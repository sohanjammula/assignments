{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d256b0-d180-42d4-8fe5-a6a8c83f42f2",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of precision and recall in the context of classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55097497-4df8-46d6-9c97-6974f7002d3a",
   "metadata": {},
   "source": [
    "In the context of classification models, precision and recall are two fundamental evaluation metrics that assess the performance of the model, particularly in binary classification tasks.\n",
    "\n",
    "Precision:\n",
    "Precision, also known as positive predictive value, measures the proportion of correctly identified positive instances out of all instances predicted as positive by the model. In other words, precision indicates the accuracy of the positive predictions made by the model.\n",
    "\n",
    "Mathematically, precision is calculated as:\n",
    "\n",
    "Precision\n",
    "=\n",
    "True Positives\n",
    "True Positives\n",
    "+\n",
    "False Positives\n",
    "Precision= \n",
    "True Positives+False Positives\n",
    "True Positives\n",
    "‚Äã\n",
    " \n",
    "\n",
    "where:\n",
    "\n",
    "True Positives (TP) are instances correctly predicted as positive.\n",
    "False Positives (FP) are instances incorrectly predicted as positive.\n",
    "Precision answers the question: \"Of all instances predicted as positive, how many were actually positive?\" A high precision value indicates that when the model predicts a positive outcome, it is correct most of the time, minimizing false positive predictions.\n",
    "\n",
    "Recall:\n",
    "Recall, also known as sensitivity or true positive rate, measures the proportion of correctly identified positive instances out of all actual positive instances in the dataset. In other words, recall quantifies the model's ability to capture all positive instances.\n",
    "\n",
    "Mathematically, recall is calculated as:\n",
    "\n",
    "Recall\n",
    "=\n",
    "True Positives\n",
    "True Positives\n",
    "+\n",
    "False Negatives\n",
    "Recall= \n",
    "True Positives+False Negatives\n",
    "True Positives\n",
    "‚Äã\n",
    " \n",
    "\n",
    "where:\n",
    "\n",
    "True Positives (TP) are instances correctly predicted as positive.\n",
    "False Negatives (FN) are instances incorrectly predicted as negative.\n",
    "Recall answers the question: \"Of all actual positive instances, how many were correctly identified by the model?\" A high recall value indicates that the model effectively captures most positive instances, minimizing false negative predictions.\n",
    "\n",
    "In summary, precision and recall provide complementary insights into the performance of a classification model. Precision emphasizes the accuracy of positive predictions, while recall emphasizes the model's ability to capture all positive instances. Both metrics are essential for evaluating the effectiveness of the model in different contexts and guiding decision-making processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c5610c-b7b1-433b-bf6b-82485e6cf3fd",
   "metadata": {},
   "source": [
    "Q2. What is the F1 score and how is it calculated? How is it different from precision and recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c647120d-56aa-4030-b137-d2073b26e065",
   "metadata": {},
   "source": [
    "\n",
    "The F1 score is a single metric that combines both precision and recall into a single value, providing a balanced measure of a classification model's performance. It is particularly useful when you want to consider both false positives and false negatives, and you want to find a balance between precision and recall.\n",
    "\n",
    "Mathematically, the F1 score is calculated as the harmonic mean of precision and recall. The harmonic mean gives more weight to lower values, which makes the F1 score sensitive to imbalances between precision and recall. The formula for calculating the F1 score is as follows:\n",
    "\n",
    "ùêπ\n",
    "1\n",
    " score\n",
    "=\n",
    "2\n",
    "√ó\n",
    "Precision\n",
    "√ó\n",
    "Recall\n",
    "Precision\n",
    "+\n",
    "Recall\n",
    "F1 score=2√ó \n",
    "Precision+Recall\n",
    "Precision√óRecall\n",
    "‚Äã\n",
    " \n",
    "\n",
    "In summary:\n",
    "\n",
    "F1 score combines both precision and recall into a single metric.\n",
    "It provides a balanced measure of a model's performance, considering both false positives and false negatives.\n",
    "F1 score reaches its best value at 1 and worst value at 0. It is always between 0 and 1.\n",
    "F1 score is particularly useful when there is an uneven class distribution (class imbalance) in the dataset.\n",
    "Compared to precision and recall:\n",
    "\n",
    "Precision focuses on the accuracy of positive predictions, while recall focuses on the model's ability to capture all positive instances.\n",
    "F1 score provides a balanced measure by considering both precision and recall, whereas precision and recall are individual metrics.\n",
    "F1 score penalizes models with imbalanced precision and recall values, making it a more comprehensive metric for evaluating classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1521a4c1-1f84-4b2b-b1e7-8cfeb9516ea3",
   "metadata": {},
   "source": [
    "Q3. What is ROC and AUC, and how are they used to evaluate the performance of classification models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1874f5-4393-499c-8185-ef2b3eb7feae",
   "metadata": {},
   "source": [
    "\n",
    "ROC (Receiver Operating Characteristic) curve and AUC (Area Under the ROC Curve) are widely used metrics for evaluating the performance of classification models, particularly in binary classification tasks. Here's what they represent and how they are used:\n",
    "\n",
    "ROC Curve:\n",
    "\n",
    "The ROC curve is a graphical plot that illustrates the diagnostic ability of a binary classification model across various threshold settings.\n",
    "It plots the true positive rate (TPR), also known as sensitivity or recall, against the false positive rate (FPR) at various threshold settings.\n",
    "TPR (True Positive Rate) is the ratio of true positives to the total number of actual positives in the dataset, while FPR (False Positive Rate) is the ratio of false positives to the total number of actual negatives in the dataset.\n",
    "AUC (Area Under the ROC Curve):\n",
    "\n",
    "AUC measures the area under the ROC curve and quantifies the overall performance of the classification model.\n",
    "AUC ranges from 0 to 1, where a model with an AUC of 1 indicates perfect discrimination (all true positives, no false positives), and a model with an AUC of 0.5 indicates no discrimination (random guessing).\n",
    "AUC provides a single scalar value that summarizes the model's performance across all possible threshold settings. Higher AUC values indicate better overall performance.\n",
    "Interpretation:\n",
    "\n",
    "The ROC curve visually displays the trade-off between sensitivity and specificity for different threshold settings.\n",
    "A model with a higher AUC value generally has better discrimination ability, meaning it can distinguish between positive and negative instances more effectively across various threshold settings.\n",
    "AUC is particularly useful for comparing the performance of different classification models or selecting the optimal threshold setting for a specific application.\n",
    "In summary, ROC curve and AUC are valuable tools for evaluating the discriminatory power of classification models and assessing their overall performance. They provide insights into the model's ability to correctly classify instances and distinguish between positive and negative cases across different threshold settings.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de861a40-aed6-4e7a-aea0-7143506e8fa2",
   "metadata": {},
   "source": [
    "Q4. How do you choose the best metric to evaluate the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c3d5b-808c-4cf6-a43d-ebe5f89da4e2",
   "metadata": {},
   "source": [
    "\n",
    "Choosing the best metric to evaluate the performance of a classification model depends on several factors, including the specific goals of the task, the characteristics of the dataset, and the preferences of stakeholders. Here are some considerations to help you choose the most appropriate metric:\n",
    "\n",
    "Task Requirements: Consider the specific requirements and objectives of the classification task. For example, in a medical diagnosis task, the emphasis may be on minimizing false negatives (maximizing recall) to avoid missing positive cases, while in a spam detection task, minimizing false positives (maximizing precision) may be more critical to avoid misclassifying legitimate emails as spam.\n",
    "\n",
    "Class Distribution: Examine the distribution of classes in the dataset. If the classes are imbalanced, accuracy may not be a reliable metric, and alternative metrics such as precision, recall, F1 score, or AUC may be more appropriate for assessing model performance.\n",
    "\n",
    "Costs of Errors: Consider the costs associated with different types of prediction errors (false positives and false negatives). Choose a metric that aligns with the relative importance of minimizing these errors in the context of the application. For example, in fraud detection, the cost of false positives (incorrectly flagging legitimate transactions as fraudulent) may be higher than the cost of false negatives (missing fraudulent transactions).\n",
    "\n",
    "Stakeholder Preferences: Consult with stakeholders or end-users to understand their preferences and priorities for model performance metrics. Stakeholders may have specific requirements or constraints that influence the choice of evaluation metrics.\n",
    "\n",
    "Model Complexity: Evaluate the complexity of the classification problem and the interpretability of the model. Some metrics may be more suitable for assessing the performance of complex models with multiple classes, while others may be better suited for simple models or binary classification tasks.\n",
    "\n",
    "Business Objectives: Consider the broader business objectives and key performance indicators (KPIs) that the classification model aims to impact. Choose metrics that are aligned with these objectives and provide meaningful insights into the model's effectiveness in achieving business goals.\n",
    "\n",
    "Ultimately, there is no one-size-fits-all metric for evaluating classification model performance. It's essential to carefully consider the specific context, requirements, and goals of the task when selecting the most appropriate metric(s) to assess model performance effectively. Additionally, it's often beneficial to report multiple metrics to provide a comprehensive understanding of the model's strengths and weaknesses from different perspectives.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee7711-8855-47d1-bbcf-5d95f895f6fa",
   "metadata": {},
   "source": [
    "Q5. Explain how logistic regression can be used for multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059c0045-a01e-4ef1-ab33-01294a8897ba",
   "metadata": {},
   "source": [
    "\n",
    "Logistic regression is inherently a binary classification algorithm, meaning it is designed to predict binary outcomes (e.g., yes/no, 1/0). However, logistic regression can be extended to handle multiclass classification tasks using various strategies. Here are some common approaches for using logistic regression in multiclass classification:\n",
    "\n",
    "One-vs-Rest (OvR) or One-vs-All (OvA):\n",
    "\n",
    "In the OvR approach, a separate logistic regression model is trained for each class, with one class treated as the positive class and all other classes combined into the negative class.\n",
    "During prediction, the model predicts the probability of each class independently, and the class with the highest probability is assigned as the final prediction.\n",
    "This approach effectively transforms the multiclass problem into multiple binary classification subproblems.\n",
    "OvR is simple to implement and can be applied with any binary classification algorithm, including logistic regression.\n",
    "Multinomial Logistic Regression:\n",
    "\n",
    "In multinomial logistic regression, also known as softmax regression, a single logistic regression model is trained to predict the probabilities of all classes simultaneously.\n",
    "The softmax function is used to convert raw model outputs (logits) into probabilities, ensuring that the predicted probabilities sum to 1 across all classes.\n",
    "During training, the model learns a separate set of weights for each class, and the loss function (e.g., cross-entropy loss) is optimized to minimize the difference between predicted and actual class probabilities.\n",
    "Multinomial logistic regression directly models the joint probability distribution of all classes and can provide more interpretable results compared to OvR.\n",
    "Regularized Logistic Regression:\n",
    "\n",
    "Regularization techniques such as Lasso (L1) or Ridge (L2) regularization can be applied to logistic regression models to prevent overfitting and improve generalization performance in multiclass classification tasks.\n",
    "Regularization penalizes large coefficients in the model, encouraging simpler models that are less prone to overfitting.\n",
    "Regularized logistic regression can help handle high-dimensional feature spaces and mitigate multicollinearity among predictors.\n",
    "Each of these approaches has its advantages and limitations, and the choice of method depends on factors such as the size of the dataset, the number of classes, the interpretability of the model, and computational considerations. Experimentation and validation on representative datasets are crucial for determining the most suitable approach for a specific multiclass classification problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c8033c-4ae7-478c-894d-09da018ebdc0",
   "metadata": {},
   "source": [
    "Q6. Describe the steps involved in an end-to-end project for multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f4943b-d48d-4090-89a8-a040294b7051",
   "metadata": {},
   "source": [
    "Problem Definition:\n",
    "\n",
    "Clearly define the problem you aim to solve with multiclass classification. Identify the classes you want to predict and the business objectives behind the classification task.\n",
    "Data Collection:\n",
    "\n",
    "Gather relevant data sources that contain information about the classes you want to predict. Ensure the data is representative of the problem domain and covers all classes adequately.\n",
    "Data Preprocessing:\n",
    "\n",
    "Handle missing values: Impute missing values or remove instances with missing data.\n",
    "Feature engineering: Select, transform, or create relevant features from the raw data to improve model performance.\n",
    "Feature scaling: Standardize or normalize numerical features to ensure they have similar scales.\n",
    "Encoding categorical variables: Convert categorical variables into numerical format using techniques like one-hot encoding.\n",
    "Exploratory Data Analysis (EDA):\n",
    "\n",
    "Analyze the distribution of classes: Understand the balance or imbalance between different classes in the dataset.\n",
    "Visualize feature distributions: Explore the relationships between features and classes using plots and statistical summaries.\n",
    "Identify outliers or anomalies: Detect and handle any outliers that may affect model performance.\n",
    "Data Splitting:\n",
    "\n",
    "Split the dataset into training, validation, and testing sets. Use the training set for model training, the validation set for hyperparameter tuning, and the testing set for final model evaluation.\n",
    "Model Selection:\n",
    "\n",
    "Choose appropriate algorithms for multiclass classification, such as logistic regression, decision trees, random forests, support vector machines, or deep learning models like neural networks.\n",
    "Consider the characteristics of the data, the complexity of the problem, and the interpretability of the models when making your selection.\n",
    "Model Training:\n",
    "\n",
    "Train the selected models using the training data. Optimize hyperparameters using techniques like grid search or random search.\n",
    "Evaluate models using appropriate evaluation metrics for multiclass classification, such as accuracy, precision, recall, F1 score, or ROC-AUC score.\n",
    "Model Evaluation:\n",
    "\n",
    "Assess the performance of trained models on the validation set. Compare the performance of different models and select the best-performing one based on chosen evaluation metrics.\n",
    "Model Fine-tuning:\n",
    "\n",
    "Fine-tune the selected model further if necessary, adjusting hyperparameters or conducting feature selection to improve performance.\n",
    "Final Model Evaluation:\n",
    "\n",
    "Evaluate the final model's performance on the testing set to assess its generalization ability on unseen data. Ensure that the model performs well and meets the business requirements.\n",
    "Model Deployment:\n",
    "\n",
    "Deploy the final trained model into production for making predictions on new, unseen data. Integrate the model into the production environment, ensuring scalability, reliability, and real-time inference capabilities.\n",
    "Monitoring and Maintenance:\n",
    "\n",
    "Monitor the deployed model's performance over time and retrain or update the model periodically as new data becomes available. Ensure ongoing maintenance to keep the model relevant and accurate.\n",
    "By following these steps systematically, you can develop an effective multiclass classification model that addresses the problem at hand and delivers actionable insights or predictio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc6b5f4-d74c-466f-aa6c-718d8606087c",
   "metadata": {},
   "source": [
    "Q7. What is model deployment and why is it important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec56a57-7b18-46b1-8b19-ea99843867e9",
   "metadata": {},
   "source": [
    "Model deployment refers to the process of integrating a trained machine learning model into a production environment where it can be used to make predictions or provide insights on new, unseen data. It involves setting up the necessary infrastructure, deploying the model code, and ensuring that it operates efficiently and reliably in real-world scenarios.\n",
    "\n",
    "Model deployment is crucial for several reasons:\n",
    "\n",
    "Operationalization: Deploying a model allows organizations to operationalize the insights gained from machine learning. It transforms a trained model from a research or experimental phase into a practical tool that can be used to make informed decisions and drive business outcomes.\n",
    "\n",
    "Scalability: Deploying a model enables it to handle large volumes of data and make predictions at scale. By deploying the model in a scalable environment, organizations can accommodate increasing workloads and growing user demands without compromising performance.\n",
    "\n",
    "Real-time Inference: Deploying a model in a production environment enables real-time inference, where predictions can be generated on-the-fly in response to user queries or events. Real-time inference is essential for applications that require immediate responses, such as fraud detection or recommendation systems.\n",
    "\n",
    "Automation: Deployed models can automate repetitive tasks and processes, saving time and resources for organizations. By integrating machine learning models into automated workflows, organizations can streamline operations and improve efficiency.\n",
    "\n",
    "Decision Support: Deployed models provide decision support by generating predictions, classifications, or recommendations based on data. These predictions can help stakeholders make informed decisions, identify opportunities, mitigate risks, and optimize processes.\n",
    "\n",
    "Continuous Improvement: Deployed models facilitate continuous improvement through feedback loops and monitoring. Organizations can collect feedback on model performance in production, analyze performance metrics, and use this information to retrain or update the model for better accuracy and relevance over time.\n",
    "\n",
    "Overall, model deployment is essential for realizing the value of machine learning models and translating them into tangible benefits for organizations. It enables organizations to leverage predictive analytics, optimize processes, and drive innovation in various domains, from healthcare and finance to e-commerce and manufacturing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dbaa76-112d-41ef-9818-5edac7006078",
   "metadata": {},
   "source": [
    "Q8. Explain how multi-cloud platforms are used for model deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261dd65-a08c-434c-9fe4-70bc0341d134",
   "metadata": {},
   "source": [
    "\n",
    "Multi-cloud platforms refer to the use of multiple cloud computing providers to host and deploy applications, including machine learning models. Here's how multi-cloud platforms are used for model deployment:\n",
    "\n",
    "Vendor Diversity: By leveraging multiple cloud providers, organizations can mitigate risks associated with vendor lock-in and dependencies on a single cloud provider. This strategy offers flexibility and resilience, allowing organizations to choose the best cloud services and features from different providers based on their specific requirements.\n",
    "\n",
    "Hybrid Deployments: Multi-cloud platforms enable organizations to deploy applications and services across multiple cloud environments, including public clouds, private clouds, and on-premises infrastructure. This hybrid approach allows organizations to balance performance, cost, security, and compliance requirements while maintaining control over their data and workloads.\n",
    "\n",
    "Redundancy and High Availability: Deploying models on multi-cloud platforms can improve redundancy and high availability by distributing workloads across geographically dispersed cloud regions or availability zones. In the event of a cloud outage or service disruption from one provider, applications can failover to another provider seamlessly, minimizing downtime and ensuring business continuity.\n",
    "\n",
    "Performance Optimization: Multi-cloud platforms offer the flexibility to optimize performance by selecting cloud services and regions that offer the best performance and latency characteristics for specific use cases. Organizations can deploy models closer to end-users or data sources, reducing latency and improving responsiveness.\n",
    "\n",
    "Cost Optimization: Multi-cloud platforms provide opportunities for cost optimization by leveraging competitive pricing, discounts, and pricing models offered by different cloud providers. Organizations can dynamically allocate resources, scale deployments, and optimize resource utilization to minimize costs while meeting performance and scalability requirements.\n",
    "\n",
    "Risk Management: Multi-cloud platforms help organizations manage risks related to data sovereignty, compliance, and regulatory requirements by distributing workloads across multiple cloud providers and geographic regions. This approach enhances data resilience, privacy, and compliance with industry-specific regulations and standards.\n",
    "\n",
    "Flexibility and Agility: Multi-cloud platforms offer flexibility and agility in deploying and managing machine learning models by providing a broad range of services, tools, and APIs for development, deployment, monitoring, and management. Organizations can leverage the best-of-breed technologies and services from different cloud providers to build scalable, resilient, and innovative applications.\n",
    "\n",
    "Overall, multi-cloud platforms offer numerous benefits for model deployment, including vendor diversity, hybrid deployments, redundancy, high availability, performance optimization, cost optimization, risk management, flexibility, and agility. By embracing multi-cloud strategies, organizations can unlock the full potential of cloud computing and machine learning to drive digital transformation and achieve business objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774e4a57-7ea4-43c6-9a6a-6a909b0d3a0a",
   "metadata": {},
   "source": [
    "Q9. Discuss the benefits and challenges of deploying machine learning models in a multi-cloud\n",
    "environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12970c68-8fc2-4639-abda-67a1f318cfd5",
   "metadata": {},
   "source": [
    "\n",
    "Deploying machine learning models in a multi-cloud environment offers several benefits and challenges. Let's explore them:\n",
    "\n",
    "Benefits:\n",
    "\n",
    "Vendor Diversity: Leveraging multiple cloud providers reduces dependency on a single vendor, mitigating the risk of vendor lock-in. It allows organizations to choose the best services and features from different providers based on their specific requirements.\n",
    "\n",
    "Redundancy and High Availability: Multi-cloud deployments enhance redundancy and high availability by distributing workloads across multiple cloud providers and geographic regions. In the event of a cloud outage or service disruption, applications can failover to another provider seamlessly, minimizing downtime and ensuring business continuity.\n",
    "\n",
    "Performance Optimization: Multi-cloud environments enable organizations to optimize performance by selecting cloud services and regions that offer the best performance and latency characteristics for specific use cases. Deploying models closer to end-users or data sources reduces latency and improves responsiveness.\n",
    "\n",
    "Cost Optimization: Multi-cloud deployments provide opportunities for cost optimization by leveraging competitive pricing, discounts, and pricing models offered by different cloud providers. Organizations can dynamically allocate resources, scale deployments, and optimize resource utilization to minimize costs while meeting performance and scalability requirements.\n",
    "\n",
    "Risk Management: Multi-cloud deployments help organizations manage risks related to data sovereignty, compliance, and regulatory requirements by distributing workloads across multiple cloud providers and geographic regions. This approach enhances data resilience, privacy, and compliance with industry-specific regulations and standards.\n",
    "\n",
    "Challenges:\n",
    "\n",
    "Complexity: Managing a multi-cloud environment introduces complexity in terms of provisioning, monitoring, security, and governance. Organizations need to invest in tools, processes, and expertise to manage heterogeneous environments effectively.\n",
    "\n",
    "Interoperability: Ensuring interoperability and compatibility between different cloud providers' services and APIs can be challenging. Organizations may encounter compatibility issues, data transfer costs, and integration complexities when migrating or replicating workloads across multiple clouds.\n",
    "\n",
    "Data Movement and Latency: Moving data between different cloud providers or regions can incur latency, bandwidth, and egress costs. Data replication, synchronization, and consistency across multiple clouds require careful planning and optimization to minimize latency and ensure data integrity.\n",
    "\n",
    "Vendor Lock-in: Despite efforts to avoid vendor lock-in, organizations may still face challenges in transitioning workloads between cloud providers due to proprietary services, APIs, or data formats. Vendor-specific features or dependencies can limit portability and interoperability across multi-cloud environments.\n",
    "\n",
    "Security and Compliance: Managing security and compliance across multiple cloud providers introduces complexity in terms of identity and access management, data protection, encryption, and regulatory compliance. Ensuring consistent security policies, controls, and monitoring across heterogeneous environments is crucial for maintaining data confidentiality and compliance.\n",
    "\n",
    "In summary, deploying machine learning models in a multi-cloud environment offers benefits such as vendor diversity, redundancy, performance optimization, cost optimization, and risk management. However, organizations must address challenges related to complexity, interoperability, data movement, latency, vendor lock-in, and security to realize the full potential of multi-cloud deployments. Effective governance, automation, and collaboration between stakeholders are essential for successful multi-cloud adoption and management.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5feebf1-6ecb-4531-8fe7-4b75b4828f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
