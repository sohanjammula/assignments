{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a692d3-ec80-4302-b052-4f462922ece3",
   "metadata": {},
   "source": [
    "#### Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40029b93-d24c-40b0-8d9f-f008900cab89",
   "metadata": {},
   "source": [
    "Bayes' theorem, named after the Reverend Thomas Bayes, is a fundamental concept in probability theory and statistics. It describes the probability of an event based on prior knowledge or conditions that might be related to the event. Mathematically, Bayes' theorem is stated as follows:\n",
    "\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    "∣\n",
    "𝐵\n",
    ")\n",
    "=\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    "∣\n",
    "𝐴\n",
    ")\n",
    "⋅\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    ")\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    ")\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(B∣A)⋅P(A)\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    "∣\n",
    "𝐵\n",
    ")\n",
    "P(A∣B) is the probability of event A occurring given that event B has occurred. This is called the posterior probability.\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    "∣\n",
    "𝐴\n",
    ")\n",
    "P(B∣A) is the probability of event B occurring given that event A has occurred. This is called the likelihood.\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    ")\n",
    "P(A) is the prior probability of event A occurring independently of event B.\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    ")\n",
    "P(B) is the probability of event B occurring independently of event A. It serves as a normalization factor.\n",
    "Bayes' theorem is often used in various fields, including machine learning, statistics, and artificial intelligence, to update beliefs or make predictions based on new evidence or observations. It provides a systematic framework for incorporating prior knowledge or assumptions into probabilistic reasoning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377038b4-7b0a-4019-870c-2dec74865238",
   "metadata": {},
   "source": [
    "#### Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a8dc3d-9759-4780-9087-0f837d11bc54",
   "metadata": {},
   "source": [
    "Bayes' theorem is a fundamental concept in probability theory and statistics. It describes the probability of an event based on prior knowledge or conditions that might be related to the event. Mathematically, Bayes' theorem is stated as follows:\n",
    "\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    "∣\n",
    "𝐵\n",
    ")\n",
    "=\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    "∣\n",
    "𝐴\n",
    ")\n",
    "⋅\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    ")\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    ")\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(B∣A)⋅P(A)\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    "∣\n",
    "𝐵\n",
    ")\n",
    "P(A∣B) is the probability of event A occurring given that event B has occurred. This is called the posterior probability.\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    "∣\n",
    "𝐴\n",
    ")\n",
    "P(B∣A) is the probability of event B occurring given that event A has occurred. This is called the likelihood.\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    ")\n",
    "P(A) is the prior probability of event A occurring independently of event B.\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    ")\n",
    "P(B) is the probability of event B occurring independently of event A. It serves as a normalization factor.\n",
    "Bayes' theorem is widely used in various fields, including machine learning, statistics, and artificial intelligence, to update beliefs or make predictions based on new evidence or observations. It provides a systematic framework for incorporating prior knowledge or assumptions into probabilistic reasoning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05364945-989b-48aa-8489-f1e7b840b6ac",
   "metadata": {},
   "source": [
    "#### Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb505c3c-8d78-4b28-9738-43550995758f",
   "metadata": {},
   "source": [
    "Bayes' theorem is used in practice in various fields and applications, including:\n",
    "\n",
    "Machine Learning: Bayes' theorem is fundamental in probabilistic machine learning algorithms, such as Naive Bayes classifiers and Bayesian networks. These algorithms use Bayes' theorem to update probabilities and make predictions based on observed data.\n",
    "\n",
    "Statistical Inference: In statistics, Bayes' theorem is used for Bayesian inference, which involves updating beliefs or probabilities about parameters of interest based on observed data. Bayesian methods provide a flexible framework for statistical modeling and hypothesis testing.\n",
    "\n",
    "Medical Diagnosis: Bayes' theorem is applied in medical diagnosis to assess the probability of a disease given certain symptoms or test results. Bayesian reasoning allows healthcare professionals to combine prior knowledge (e.g., prevalence of the disease) with new evidence (e.g., patient symptoms) to make accurate diagnostic decisions.\n",
    "\n",
    "Spam Filtering: In email spam filtering, Bayes' theorem is used in algorithms like the Naive Bayes classifier to classify emails as spam or non-spam. The classifier calculates the probability that an email is spam given the presence of certain words or features, based on historical data.\n",
    "\n",
    "Risk Assessment: Bayes' theorem is employed in risk assessment and decision-making processes across various industries, including finance, insurance, and engineering. It helps quantify the likelihood of different outcomes and make informed decisions based on available information.\n",
    "\n",
    "Natural Language Processing: In natural language processing tasks, such as language modeling and text classification, Bayes' theorem is used to estimate the probability of a sequence of words or the likelihood of a document belonging to a particular category.\n",
    "\n",
    "Overall, Bayes' theorem provides a powerful framework for reasoning under uncertainty and is widely applied in diverse fields to make predictions, infer causal relationships, and update beliefs based on new evidence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644c2bc-9944-41f7-927b-8bdbd43f51ff",
   "metadata": {},
   "source": [
    "#### Q4. What is the relationship between Bayes' theorem and conditional probability?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf405dd-5219-4e77-90d7-da7f1962d2b1",
   "metadata": {},
   "source": [
    "Bayes' theorem and conditional probability are closely related concepts in probability theory. Conditional probability refers to the probability of an event occurring given that another event has already occurred. Bayes' theorem provides a systematic way to calculate conditional probabilities using prior knowledge or information.\n",
    "\n",
    "The relationship between Bayes' theorem and conditional probability can be understood as follows:\n",
    "\n",
    "Bayes' Theorem: Bayes' theorem states the relationship between the conditional probability of two events. Mathematically, it is represented as:\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    "∣\n",
    "𝐵\n",
    ")\n",
    "=\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    "∣\n",
    "𝐴\n",
    ")\n",
    "⋅\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    ")\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    ")\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(B∣A)⋅P(A)\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    "∣\n",
    "𝐵\n",
    ")\n",
    "P(A∣B) is the probability of event A occurring given that event B has occurred (posterior probability).\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    "∣\n",
    "𝐴\n",
    ")\n",
    "P(B∣A) is the probability of event B occurring given that event A has occurred (likelihood).\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    ")\n",
    "P(A) is the prior probability of event A occurring independently of event B.\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    ")\n",
    "P(B) is the probability of event B occurring independently of event A.\n",
    "Conditional Probability: Conditional probability is a fundamental concept in probability theory that quantifies the likelihood of an event given that another event has already occurred. It is represented as \n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    "∣\n",
    "𝐵\n",
    ")\n",
    "P(A∣B), where:\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    "∣\n",
    "𝐵\n",
    ")\n",
    "P(A∣B) is the probability of event A occurring given that event B has occurred.\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    ")\n",
    "P(B) is the probability of event B occurring.\n",
    "Bayes' theorem and conditional probability are related in that Bayes' theorem allows us to calculate the posterior probability \n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    "∣\n",
    "𝐵\n",
    ")\n",
    "P(A∣B) based on the likelihood \n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    "∣\n",
    "𝐴\n",
    ")\n",
    "P(B∣A), the prior probability \n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    ")\n",
    "P(A), and the probability of event B \n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    ")\n",
    "P(B). In other words, Bayes' theorem provides a method to update our beliefs about the probability of an event given new evidence or information.\n",
    "\n",
    "Overall, Bayes' theorem and conditional probability are essential concepts in probabilistic reasoning, and understanding their relationship is fundamental in various fields, including statistics, machine learning, and decision theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dab18b-d1fb-42b8-8651-bd8f875bb35b",
   "metadata": {},
   "source": [
    "#### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69399347-7a6c-4384-bbed-95350bcc234e",
   "metadata": {},
   "source": [
    "Choosing the appropriate type of Naive Bayes classifier for a given problem depends on several factors, including the characteristics of the data and the assumptions made by each classifier. The three main types of Naive Bayes classifiers are Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. Here are some considerations for selecting the appropriate type:\n",
    "\n",
    "Nature of the Features:\n",
    "\n",
    "Gaussian Naive Bayes: This classifier assumes that the features follow a Gaussian (normal) distribution. It is suitable for continuous numerical features that approximately follow a bell-shaped curve.\n",
    "Multinomial Naive Bayes: This classifier is designed for features that represent counts or frequencies, such as word counts in text classification tasks. It works well with discrete features that are integer counts.\n",
    "Bernoulli Naive Bayes: This classifier is suitable for binary or Boolean features, where each feature represents the presence or absence of a particular attribute. It is commonly used in text classification tasks with binary features indicating the presence or absence of words.\n",
    "Data Distribution:\n",
    "\n",
    "If the features in the dataset closely follow the assumptions of Gaussian distribution, Gaussian Naive Bayes may be appropriate.\n",
    "If the features are discrete or represent counts/frequencies, Multinomial or Bernoulli Naive Bayes may be more suitable, depending on whether the features are binary or multinomial.\n",
    "Size of the Dataset:\n",
    "\n",
    "Gaussian Naive Bayes can handle small to moderately sized datasets effectively.\n",
    "Multinomial and Bernoulli Naive Bayes are often used in large-scale text classification tasks with high-dimensional feature spaces.\n",
    "Sparsity of Data:\n",
    "\n",
    "If the dataset is sparse, with many zero values in the feature matrix (e.g., in text classification with a large vocabulary), Bernoulli Naive Bayes may be more appropriate as it handles binary features efficiently.\n",
    "Model Interpretability:\n",
    "\n",
    "Consider the interpretability of the model outputs. Gaussian Naive Bayes provides probability estimates assuming a Gaussian distribution, while Multinomial and Bernoulli Naive Bayes provide likelihood estimates based on feature counts.\n",
    "Cross-validation Performance:\n",
    "\n",
    "Finally, it's essential to evaluate the performance of each type of Naive Bayes classifier using cross-validation or other validation techniques on your specific dataset. Choose the classifier that performs best in terms of classification accuracy or other relevant metrics.\n",
    "In summary, the choice of Naive Bayes classifier depends on the nature of the features, data distribution, size of the dataset, sparsity of data, interpretability requirements, and cross-validation performance. It's often helpful to experiment with different types of Naive Bayes classifiers and select the one that best fits the characteristics of the data and the requirements of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c82a707-dee5-4744-9782-2a522084e838",
   "metadata": {},
   "source": [
    "#### '''Q6. Assignment:\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "each feature value for each class:\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "A 3 3 4 4 3 3 3\n",
    "B 2 2 1 2 2 2 3\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "to belong to?'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e681e9c-2074-446f-9bd5-b30c394d0ee0",
   "metadata": {},
   "source": [
    "To predict the class of the new instance with features \n",
    "𝑋\n",
    "1\n",
    "=\n",
    "3\n",
    "X \n",
    "1\n",
    "​\n",
    " =3 and \n",
    "𝑋\n",
    "2\n",
    "=\n",
    "4\n",
    "X \n",
    "2\n",
    "​\n",
    " =4 using Naive Bayes classification, we'll calculate the posterior probabilities for each class \n",
    "𝐴\n",
    "A and \n",
    "𝐵\n",
    "B based on the given frequencies and assuming equal prior probabilities for each class.\n",
    "\n",
    "The Naive Bayes classifier predicts the class with the highest posterior probability.\n",
    "\n",
    "Given:\n",
    "\n",
    "Prior probability for each class is assumed to be equal.\n",
    "The features \n",
    "𝑋\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    "  and \n",
    "𝑋\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    "  are independent (Naive Bayes assumption).\n",
    "We'll calculate the likelihood \n",
    "𝑃\n",
    "(\n",
    "𝑋\n",
    "1\n",
    "=\n",
    "3\n",
    "∣\n",
    "𝐴\n",
    ")\n",
    "P(X \n",
    "1\n",
    "​\n",
    " =3∣A), \n",
    "𝑃\n",
    "(\n",
    "𝑋\n",
    "2\n",
    "=\n",
    "4\n",
    "∣\n",
    "𝐴\n",
    ")\n",
    "P(X \n",
    "2\n",
    "​\n",
    " =4∣A), \n",
    "𝑃\n",
    "(\n",
    "𝑋\n",
    "1\n",
    "=\n",
    "3\n",
    "∣\n",
    "𝐵\n",
    ")\n",
    "P(X \n",
    "1\n",
    "​\n",
    " =3∣B), and \n",
    "𝑃\n",
    "(\n",
    "𝑋\n",
    "2\n",
    "=\n",
    "4\n",
    "∣\n",
    "𝐵\n",
    ")\n",
    "P(X \n",
    "2\n",
    "​\n",
    " =4∣B) for each class \n",
    "𝐴\n",
    "A and \n",
    "𝐵\n",
    "B, and then use Bayes' theorem to calculate the posterior probabilities.\n",
    "\n",
    "Let's calculate:\n",
    "\n",
    "For class A:\n",
    "\n",
    "𝑃\n",
    "(\n",
    "𝑋\n",
    "1\n",
    "=\n",
    "3\n",
    "∣\n",
    "𝐴\n",
    ")\n",
    "=\n",
    "4\n",
    "13\n",
    "P(X \n",
    "1\n",
    "​\n",
    " =3∣A)= \n",
    "13\n",
    "4\n",
    "​\n",
    " \n",
    "𝑃\n",
    "(\n",
    "𝑋\n",
    "2\n",
    "=\n",
    "4\n",
    "∣\n",
    "𝐴\n",
    ")\n",
    "=\n",
    "3\n",
    "13\n",
    "P(X \n",
    "2\n",
    "​\n",
    " =4∣A)= \n",
    "13\n",
    "3\n",
    "​\n",
    " \n",
    "For class B:\n",
    "\n",
    "𝑃\n",
    "(\n",
    "𝑋\n",
    "1\n",
    "=\n",
    "3\n",
    "∣\n",
    "𝐵\n",
    ")\n",
    "=\n",
    "1\n",
    "9\n",
    "P(X \n",
    "1\n",
    "​\n",
    " =3∣B)= \n",
    "9\n",
    "1\n",
    "​\n",
    " \n",
    "𝑃\n",
    "(\n",
    "𝑋\n",
    "2\n",
    "=\n",
    "4\n",
    "∣\n",
    "𝐵\n",
    ")\n",
    "=\n",
    "3\n",
    "9\n",
    "P(X \n",
    "2\n",
    "​\n",
    " =4∣B)= \n",
    "9\n",
    "3\n",
    "​\n",
    " \n",
    "Assuming equal prior probabilities for each class, the posterior probabilities are proportional to the product of the likelihood and the prior probability.\n",
    "\n",
    "Let's calculate the posterior probabilities for each class:\n",
    "\n",
    "For class A:\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    "∣\n",
    "𝑋\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "𝑋\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "∝\n",
    "𝑃\n",
    "(\n",
    "𝑋\n",
    "1\n",
    "=\n",
    "3\n",
    "∣\n",
    "𝐴\n",
    ")\n",
    "⋅\n",
    "𝑃\n",
    "(\n",
    "𝑋\n",
    "2\n",
    "=\n",
    "4\n",
    "∣\n",
    "𝐴\n",
    ")\n",
    "P(A∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)∝P(X \n",
    "1\n",
    "​\n",
    " =3∣A)⋅P(X \n",
    "2\n",
    "​\n",
    " =4∣A)\n",
    "\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    "∣\n",
    "𝑋\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "𝑋\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "∝\n",
    "4\n",
    "13\n",
    "×\n",
    "3\n",
    "13\n",
    "P(A∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)∝ \n",
    "13\n",
    "4\n",
    "​\n",
    " × \n",
    "13\n",
    "3\n",
    "​\n",
    " \n",
    "\n",
    "For class B:\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    "∣\n",
    "𝑋\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "𝑋\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "∝\n",
    "𝑃\n",
    "(\n",
    "𝑋\n",
    "1\n",
    "=\n",
    "3\n",
    "∣\n",
    "𝐵\n",
    ")\n",
    "⋅\n",
    "𝑃\n",
    "(\n",
    "𝑋\n",
    "2\n",
    "=\n",
    "4\n",
    "∣\n",
    "𝐵\n",
    ")\n",
    "P(B∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)∝P(X \n",
    "1\n",
    "​\n",
    " =3∣B)⋅P(X \n",
    "2\n",
    "​\n",
    " =4∣B)\n",
    "\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    "∣\n",
    "𝑋\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "𝑋\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "∝\n",
    "1\n",
    "9\n",
    "×\n",
    "3\n",
    "9\n",
    "P(B∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)∝ \n",
    "9\n",
    "1\n",
    "​\n",
    " × \n",
    "9\n",
    "3\n",
    "​\n",
    " \n",
    "\n",
    "Now, we normalize the probabilities to sum to 1:\n",
    "\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    "∣\n",
    "𝑋\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "𝑋\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "=\n",
    "4\n",
    "13\n",
    "×\n",
    "3\n",
    "13\n",
    "4\n",
    "13\n",
    "×\n",
    "3\n",
    "13\n",
    "+\n",
    "1\n",
    "9\n",
    "×\n",
    "3\n",
    "9\n",
    "P(A∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)= \n",
    "13\n",
    "4\n",
    "​\n",
    " × \n",
    "13\n",
    "3\n",
    "​\n",
    " + \n",
    "9\n",
    "1\n",
    "​\n",
    " × \n",
    "9\n",
    "3\n",
    "​\n",
    " \n",
    "13\n",
    "4\n",
    "​\n",
    " × \n",
    "13\n",
    "3\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    "∣\n",
    "𝑋\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "𝑋\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "=\n",
    "1\n",
    "9\n",
    "×\n",
    "3\n",
    "9\n",
    "4\n",
    "13\n",
    "×\n",
    "3\n",
    "13\n",
    "+\n",
    "1\n",
    "9\n",
    "×\n",
    "3\n",
    "9\n",
    "P(B∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)= \n",
    "13\n",
    "4\n",
    "​\n",
    " × \n",
    "13\n",
    "3\n",
    "​\n",
    " + \n",
    "9\n",
    "1\n",
    "​\n",
    " × \n",
    "9\n",
    "3\n",
    "​\n",
    " \n",
    "9\n",
    "1\n",
    "​\n",
    " × \n",
    "9\n",
    "3\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "Let's calculate:\n",
    "\n",
    "𝑃\n",
    "(\n",
    "𝐴\n",
    "∣\n",
    "𝑋\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "𝑋\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "≈\n",
    "36\n",
    "91\n",
    "P(A∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)≈ \n",
    "91\n",
    "36\n",
    "​\n",
    " \n",
    "\n",
    "𝑃\n",
    "(\n",
    "𝐵\n",
    "∣\n",
    "𝑋\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "𝑋\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "≈\n",
    "55\n",
    "91\n",
    "P(B∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)≈ \n",
    "91\n",
    "55\n",
    "​\n",
    " \n",
    "\n",
    "Therefore, the Naive Bayes classifier would predict the new instance to belong to class B, as it has the higher posterior probability.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10b1c260-7347-45c9-a739-b8fbc0f43e88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior probability of class A: 0.597105864432597\n",
      "Posterior probability of class B: 0.4028941355674029\n",
      "Predicted class: A\n"
     ]
    }
   ],
   "source": [
    "# Define the class frequencies for each feature value\n",
    "class_frequencies = {\n",
    "    'A': {'X1=1': 3, 'X1=2': 3, 'X1=3': 4, 'X2=1': 4, 'X2=2': 3, 'X2=3': 3, 'X2=4': 3},\n",
    "    'B': {'X1=1': 2, 'X1=2': 2, 'X1=3': 1, 'X2=1': 2, 'X2=2': 2, 'X2=3': 2, 'X2=4': 3}\n",
    "}\n",
    "\n",
    "# Calculate the likelihood for each feature value for class A\n",
    "likelihood_A_X1_3 = class_frequencies['A']['X1=3'] / sum(class_frequencies['A'].values())\n",
    "likelihood_A_X2_4 = class_frequencies['A']['X2=4'] / sum(class_frequencies['A'].values())\n",
    "\n",
    "# Calculate the likelihood for each feature value for class B\n",
    "likelihood_B_X1_3 = class_frequencies['B']['X1=3'] / sum(class_frequencies['B'].values())\n",
    "likelihood_B_X2_4 = class_frequencies['B']['X2=4'] / sum(class_frequencies['B'].values())\n",
    "\n",
    "# Calculate the prior probabilities (assuming equal priors)\n",
    "prior_A = 1 / 2  # Equal prior probability for class A\n",
    "prior_B = 1 / 2  # Equal prior probability for class B\n",
    "\n",
    "# Calculate the unnormalized posterior probabilities\n",
    "posterior_A_unnormalized = likelihood_A_X1_3 * likelihood_A_X2_4 * prior_A\n",
    "posterior_B_unnormalized = likelihood_B_X1_3 * likelihood_B_X2_4 * prior_B\n",
    "\n",
    "# Normalize the posterior probabilities\n",
    "normalization_factor = posterior_A_unnormalized + posterior_B_unnormalized\n",
    "posterior_A = posterior_A_unnormalized / normalization_factor\n",
    "posterior_B = posterior_B_unnormalized / normalization_factor\n",
    "\n",
    "print(\"Posterior probability of class A:\", posterior_A)\n",
    "print(\"Posterior probability of class B:\", posterior_B)\n",
    "\n",
    "# Predict the class with the highest posterior probability\n",
    "predicted_class = 'A' if posterior_A > posterior_B else 'B'\n",
    "print(\"Predicted class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf6e00-ed7a-4878-904f-99b557a6d1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
