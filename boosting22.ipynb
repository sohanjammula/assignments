{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174544dd-bf48-4fbd-b358-9abf1ee3ba4b",
   "metadata": {},
   "source": [
    "## Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b9228e-3d7d-453b-9d08-578eba75a77f",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression\n",
    "\n",
    "Gradient Boosting Regression is a machine learning technique used for regression tasks, where the goal is to predict continuous numeric values. It is an ensemble learning method that builds a strong predictive model by sequentially adding weak learners (typically decision trees) to the ensemble.\n",
    "\n",
    "## Workflow:\n",
    "\n",
    "1. **Initialization**: The process starts with an initial prediction, which is often the mean (or median) of the target variable for all training samples.\n",
    "\n",
    "2. **Building the Ensemble**:\n",
    "   - A weak learner (usually a decision tree) is trained to predict the residuals (the difference between the actual target values and the current predictions).\n",
    "   - The residuals are used as the new target variable for training the next weak learner.\n",
    "   - Each subsequent weak learner is trained to predict the residuals of the previous model.\n",
    "\n",
    "3. **Combining Predictions**:\n",
    "   - The predictions of all weak learners are combined to make the final prediction.\n",
    "   - The final prediction is the sum of the initial prediction and the predictions of all weak learners.\n",
    "\n",
    "4. **Gradient Descent Optimization**:\n",
    "   - During the training process, instead of directly fitting the residuals, Gradient Boosting uses gradient descent optimization to minimize a loss function (e.g., mean squared error).\n",
    "   - The gradients of the loss function with respect to the predictions are computed, and the subsequent weak learner is trained to predict these gradients.\n",
    "\n",
    "5. **Regularization**:\n",
    "   - Gradient Boosting Regression often includes regularization techniques to prevent overfitting, such as limiting the maximum depth of the decision trees, adding a shrinkage parameter (learning rate), and using subsampling of the data.\n",
    "\n",
    "6. **Stopping Criteria**:\n",
    "   - The training process continues until a predefined number of weak learners are added to the ensemble or until a stopping criterion (e.g., no improvement on the validation set) is met.\n",
    "\n",
    "Gradient Boosting Regression is known for its high predictive accuracy and robustness to overfitting. It can effectively capture complex nonlinear relationships in the data and is widely used in various regression tasks, such as predicting house prices, stock prices, and customer churn rates. Popular implementations of Gradient Boosting Regression include XGBoost, LightGBM, and CatBoost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8c424-aa34-4a47-9180-7c926993712d",
   "metadata": {},
   "source": [
    "## Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6526b25-1375-4104-893d-52fd1c2eda42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 5.626315813710226\n",
      "R-squared: 0.9179828296718999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth=3):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "        \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        if depth == self.max_depth or len(np.unique(y)) == 1:\n",
    "            return np.mean(y)\n",
    "        else:\n",
    "            feature_idx, threshold, split_X_left, split_X_right, split_y_left, split_y_right = self._find_best_split(X, y)\n",
    "            if feature_idx is None:\n",
    "                return np.mean(y)\n",
    "            else:\n",
    "                tree = {}\n",
    "                tree['feature_idx'] = feature_idx\n",
    "                tree['threshold'] = threshold\n",
    "                tree['left'] = self._build_tree(split_X_left, split_y_left, depth + 1)\n",
    "                tree['right'] = self._build_tree(split_X_right, split_y_right, depth + 1)\n",
    "                return tree\n",
    "                \n",
    "    def _find_best_split(self, X, y):\n",
    "        best_mse = float('inf')\n",
    "        feature_idx, threshold = None, None\n",
    "        split_X_left, split_X_right = None, None  # Initialize variables here\n",
    "        split_y_left, split_y_right = None, None  # Initialize variables here\n",
    "        for i in range(X.shape[1]):\n",
    "            for val in np.unique(X[:, i]):\n",
    "                left_idx = X[:, i] <= val\n",
    "                right_idx = X[:, i] > val\n",
    "                if len(np.unique(y[left_idx])) < 2 or len(np.unique(y[right_idx])) < 2:\n",
    "                    continue\n",
    "                mse = self._mse(y[left_idx]) + self._mse(y[right_idx])\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    feature_idx, threshold = i, val\n",
    "                    split_X_left, split_X_right = X[left_idx], X[right_idx]\n",
    "                    split_y_left, split_y_right = y[left_idx], y[right_idx]\n",
    "        return feature_idx, threshold, split_X_left, split_X_right, split_y_left, split_y_right\n",
    "                \n",
    "    def _mse(self, y):\n",
    "        return np.mean((y - np.mean(y))**2)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "    \n",
    "    def _traverse_tree(self, x, tree):\n",
    "        if isinstance(tree, dict):\n",
    "            if x[tree['feature_idx']] <= tree['threshold']:\n",
    "                return self._traverse_tree(x, tree['left'])\n",
    "            else:\n",
    "                return self._traverse_tree(x, tree['right'])\n",
    "        else:\n",
    "            return tree\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        residuals = y.copy()\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            predictions = tree.predict(X)\n",
    "            residuals -= self.learning_rate * predictions\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(len(X))\n",
    "        for tree in self.trees:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Example usage:\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() + np.random.randn(100) * 2\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test = X[:80], X[80:]\n",
    "y_train, y_test = y[:80], y[80:]\n",
    "\n",
    "# Train Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ce032-b5b1-43ce-8af8-e085a7b34f2e",
   "metadata": {},
   "source": [
    "## Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8524ff1-dcc2-4a30-88f2-9ea4268bea36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class SklearnCompatibleGradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        residuals = y.copy()\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            predictions = tree.predict(X)\n",
    "            residuals -= self.learning_rate * predictions\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(len(X))\n",
    "        for tree in self.trees:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'max_depth': self.max_depth\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b3ee99-8c1c-403f-987b-982aaa27ce7b",
   "metadata": {},
   "source": [
    "## Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461223bd-7630-44e1-88f2-f028a177d9fe",
   "metadata": {},
   "source": [
    "# Weak Learner in Gradient Boosting\n",
    "\n",
    "In Gradient Boosting, a weak learner refers to a base model that is used as a component within the ensemble learning framework. Unlike in traditional machine learning methods where the emphasis is on building a single highly accurate model, in boosting algorithms like Gradient Boosting, the focus is on sequentially adding weak learners to create a strong predictive model.\n",
    "\n",
    "A weak learner is typically a simple model that performs slightly better than random guessing on a given problem. The key characteristic of a weak learner is that it has a performance that is only slightly better than chance for binary classification tasks (e.g., accuracy slightly above 50%). For regression tasks, a weak learner may produce predictions that are only slightly correlated with the true target values.\n",
    "\n",
    "In the context of Gradient Boosting, decision trees are commonly used as weak learners, although other types of models such as linear models or shallow neural networks can also be used. Each weak learner is trained on the residuals (the difference between the true target values and the current predictions) of the ensemble model built so far. By iteratively adding weak learners and adjusting their predictions based on the errors of the previous models, Gradient Boosting is able to gradually improve the overall performance of the ensemble.\n",
    "\n",
    "Despite being individually weak, the combination of multiple weak learners through boosting results in a strong ensemble model that can achieve high predictive accuracy. This is because each weak learner focuses on different aspects of the data, and their collective decision-making leads to robust predictions. The sequential nature of boosting allows the algorithm to effectively adapt to complex patterns and relationships present in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b27dbb-4483-4bb6-957f-56c2a4de3e61",
   "metadata": {},
   "source": [
    "## Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39828044-5539-42a6-bdb0-1d7b66c4deed",
   "metadata": {},
   "source": [
    "# Intuition Behind Gradient Boosting Algorithm\n",
    "\n",
    "The Gradient Boosting algorithm is an ensemble learning method that combines the predictions of multiple weak learners (often decision trees) to create a strong predictive model. Here's the intuition behind how Gradient Boosting works:\n",
    "\n",
    "1. **Ensemble Learning**: Gradient Boosting belongs to the ensemble learning family, where multiple weak learners are combined to create a more accurate and robust model than any individual learner.\n",
    "\n",
    "2. **Sequential Training**: Unlike bagging methods like Random Forest where weak learners are trained independently in parallel, Gradient Boosting trains weak learners sequentially. Each new learner is trained to correct the errors (residuals) of the existing ensemble.\n",
    "\n",
    "3. **Gradient Descent Optimization**: Gradient Boosting uses gradient descent optimization to minimize a loss function (e.g., mean squared error for regression tasks). At each iteration, the algorithm computes the gradients of the loss function with respect to the predictions of the current ensemble model, and updates the parameters of the weak learner in the direction that reduces the error.\n",
    "\n",
    "4. **Gradient Boosting with Decision Trees**: Decision trees are commonly used as weak learners in Gradient Boosting. Each decision tree is trained to predict the residuals (errors) of the previous ensemble, and the predictions of all trees are combined to make the final prediction.\n",
    "\n",
    "5. **Shrinkage (Learning Rate)**: Gradient Boosting often includes a shrinkage parameter (learning rate) to control the contribution of each weak learner to the ensemble. A smaller learning rate results in more conservative updates to the ensemble, which can improve generalization performance and help prevent overfitting.\n",
    "\n",
    "6. **Regularization**: Gradient Boosting typically employs regularization techniques such as limiting the maximum depth of individual trees, using subsampling of the data, and adding constraints on the complexity of the weak learners to prevent overfitting.\n",
    "\n",
    "Overall, the intuition behind Gradient Boosting is to iteratively improve the ensemble model by fitting new weak learners to the errors of the previous ensemble, using gradient descent optimization to minimize the loss function. This allows Gradient Boosting to effectively capture complex relationships in the data and achieve high predictive accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7c826-4b01-4e24-989d-6a147117ce3f",
   "metadata": {},
   "source": [
    "## Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afce8c6-7021-45cd-8bf8-d419a28654c9",
   "metadata": {},
   "source": [
    "# Building an Ensemble of Weak Learners in Gradient Boosting\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners sequentially. Here's how it works:\n",
    "\n",
    "1. **Initialization**: The process starts with an initial prediction, which is often the mean (or median) of the target variable for all training samples.\n",
    "\n",
    "2. **First Weak Learner**: The first weak learner (often a decision tree) is trained to predict the residuals (the difference between the actual target values and the initial predictions). This weak learner captures the patterns in the data that were not captured by the initial prediction.\n",
    "\n",
    "3. **Subsequent Weak Learners**: Each subsequent weak learner is trained to predict the residuals of the current ensemble model (i.e., the difference between the actual target values and the predictions of the ensemble so far). These weak learners are trained sequentially, and each one focuses on reducing the errors made by the previous ensemble.\n",
    "\n",
    "4. **Combining Predictions**: The predictions of all weak learners are combined to make the final prediction. Typically, the final prediction is the sum of the initial prediction and the predictions of all weak learners. By combining the predictions of multiple weak learners, Gradient Boosting produces a strong ensemble model that captures complex relationships in the data.\n",
    "\n",
    "5. **Gradient Descent Optimization**: During the training process, Gradient Boosting uses gradient descent optimization to minimize a loss function (e.g., mean squared error). At each iteration, the algorithm computes the negative gradient of the loss function with respect to the predictions of the current ensemble model. The subsequent weak learner is then trained to predict the negative gradient, effectively moving the ensemble model in the direction that reduces the error.\n",
    "\n",
    "6. **Regularization**: Gradient Boosting often includes regularization techniques to prevent overfitting, such as limiting the maximum depth of individual trees, using subsampling of the data, and adding constraints on the complexity of the weak learners.\n",
    "\n",
    "By iteratively adding weak learners and adjusting their predictions based on the errors of the previous ensemble, Gradient Boosting is able to gradually improve the overall performance of the ensemble and produce highly accurate predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb2a6b3-0d2b-4f4c-9d26-e0b23f534eeb",
   "metadata": {},
   "source": [
    "## Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f2350-8563-4ea3-b0b6-8a102c34694c",
   "metadata": {},
   "source": [
    "# Steps Involved in Constructing the Mathematical Intuition of Gradient Boosting Algorithm\n",
    "\n",
    "1. **Initialize with a Constant**: Start with an initial prediction, often the mean (or median) of the target variable for all training samples.\n",
    "\n",
    "2. **Compute Pseudo-Residuals**: Calculate the pseudo-residuals, which are the negative gradients of the loss function with respect to the current predictions.\n",
    "\n",
    "3. **Fit a Weak Learner**: Train a weak learner (e.g., decision tree) to predict the pseudo-residuals. This weak learner captures the patterns in the data that were not captured by the initial prediction.\n",
    "\n",
    "4. **Update Predictions**: Update the predictions by adding a fraction (learning rate) of the predictions of the weak learner to the current predictions. This step gradually improves the overall predictions of the ensemble.\n",
    "\n",
    "5. **Repeat Steps 2-4**: Iterate the process by computing new pseudo-residuals based on the updated predictions and fitting a new weak learner to predict these pseudo-residuals. Each weak learner focuses on correcting the errors made by the ensemble so far.\n",
    "\n",
    "6. **Combine Predictions**: Combine the predictions of all weak learners to make the final prediction. Typically, the final prediction is the sum of the initial prediction and the predictions of all weak learners.\n",
    "\n",
    "By following these steps, Gradient Boosting constructs an ensemble of weak learners that work together to make highly accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd802d-be5d-47d6-8cf7-775b0bde1023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
