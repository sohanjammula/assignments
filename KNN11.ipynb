{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b012e7d2-e7bf-4173-8a42-3ef85348ff37",
   "metadata": {},
   "source": [
    "## Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea651c16-e7a6-4d4f-b0b8-18392970e5dc",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) Algorithm\n",
    "\n",
    "## Overview\n",
    "The K-Nearest Neighbors (KNN) algorithm is a non-parametric, lazy learning algorithm that is widely used for classification and regression tasks. It operates by finding the `k` training samples that are closest in distance to a new input sample and using these neighbors to make predictions.\n",
    "\n",
    "## How It Works\n",
    "1. **Training Phase**: \n",
    "   - In KNN, the training phase involves storing the feature vectors and corresponding labels of the training data.\n",
    "   - No explicit model is built during training, which is why it is considered a \"lazy\" learning algorithm.\n",
    "\n",
    "2. **Prediction Phase**:\n",
    "   - For classification:\n",
    "     1. A new input sample is given.\n",
    "     2. The algorithm computes the distance between the new sample and all training samples.\n",
    "     3. The `k` nearest training samples (neighbors) are identified based on the computed distances.\n",
    "     4. The most common label (majority vote) among these `k` neighbors is assigned as the prediction for the new sample.\n",
    "   - For regression:\n",
    "     1. Similar steps are followed to find the `k` nearest neighbors.\n",
    "     2. The average (or weighted average) of the labels of these `k` neighbors is computed and assigned as the prediction for the new sample.\n",
    "\n",
    "## Distance Metrics\n",
    "- Commonly used distance metrics include Euclidean distance, Manhattan distance, and Minkowski distance.\n",
    "- The choice of distance metric can affect the performance of the algorithm.\n",
    "\n",
    "## Hyperparameters\n",
    "- `k` (number of neighbors): This is a crucial hyperparameter that determines how many neighbors will be considered when making predictions. A small value of `k` makes the model sensitive to noise, while a large value of `k` makes it more stable but potentially less sensitive to local patterns.\n",
    "- Distance metric: The metric used to measure the distance between samples (e.g., Euclidean, Manhattan).\n",
    "\n",
    "## Advantages\n",
    "- Simple to understand and implement.\n",
    "- Effective for small datasets with fewer features.\n",
    "- No training phase, making it fast to deploy for predictions.\n",
    "\n",
    "## Limitations\n",
    "- Computationally expensive during prediction, especially with large datasets, as it involves calculating the distance to all training samples.\n",
    "- Sensitive to the scale of data; features with larger ranges can dominate the distance computation, so feature scaling is often required.\n",
    "- The choice of `k` and distance metric can significantly affect performance.\n",
    "\n",
    "## Applications\n",
    "- KNN is widely used in various applications such as image recognition, recommendation systems, and anomaly detection.\n",
    "\n",
    "In summary, the K-Nearest Neighbors algorithm is a versatile and easy-to-understand machine learning algorithm that can be applied to both classification and regression problems. Its effectiveness depends on the careful choice of `k` and distance metric, as well as preprocessing steps like feature scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b03a32-3360-4f61-b3fc-115d0ca37cdb",
   "metadata": {},
   "source": [
    "## Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31800bdd-66fe-48e0-882b-c90fa2a1e51e",
   "metadata": {},
   "source": [
    "# Choosing the Value of K in KNN\n",
    "\n",
    "Selecting the optimal value of \\( K \\) is essential for achieving good performance with the K-Nearest Neighbors (KNN) algorithm. Here are some methods and considerations for choosing the value of \\( K \\):\n",
    "\n",
    "## Methods for Choosing \\( K \\)\n",
    "\n",
    "1. **Cross-Validation**:\n",
    "   - Use k-fold cross-validation to evaluate the performance of the KNN model for different values of \\( K \\).\n",
    "   - Split the training data into \\( k \\) folds, train the model on \\( k-1 \\) folds, and validate it on the remaining fold.\n",
    "   - Repeat this process for different values of \\( K \\) and select the value that yields the best average performance (e.g., highest accuracy for classification or lowest mean squared error for regression).\n",
    "\n",
    "2. **Elbow Method**:\n",
    "   - Plot the error rate (or accuracy) against different values of \\( K \\).\n",
    "   - Look for an \"elbow point\" where the error rate starts to level off. The value of \\( K \\) at this point is often a good choice.\n",
    "\n",
    "3. **Rule of Thumb**:\n",
    "   - A common heuristic is to set \\( K \\) to the square root of the number of training samples: \\( K \\approx \\sqrt{N} \\).\n",
    "   - This is a simple starting point, but it should be refined using cross-validation or other techniques.\n",
    "\n",
    "## Considerations for Choosing \\( K \\)\n",
    "\n",
    "1. **Small \\( K \\) Values**:\n",
    "   - A small value of \\( K \\) (e.g., \\( K=1 \\)) can lead to a model that is sensitive to noise in the data, potentially causing overfitting.\n",
    "   - The decision boundary may be very flexible and may capture the noise in the training data.\n",
    "\n",
    "2. **Large \\( K \\) Values**:\n",
    "   - A large value of \\( K \\) can smooth out the decision boundary, making the model less sensitive to noise but potentially underfitting.\n",
    "   - The predictions will be based on a larger set of neighbors, which can average out the noise but may also blur important patterns.\n",
    "\n",
    "3. **Odd Values for Classification**:\n",
    "   - When dealing with binary classification problems, it is often useful to choose an odd value for \\( K \\) to avoid ties in the voting process.\n",
    "\n",
    "4. **Feature Scaling**:\n",
    "   - Ensure that features are scaled (e.g., using standardization or normalization) so that the distance metric is not dominated by features with larger ranges.\n",
    "   - Properly scaled features help in obtaining a more reliable choice of \\( K \\).\n",
    "\n",
    "## Practical Steps\n",
    "1. **Split the Dataset**:\n",
    "   - Divide the dataset into training and validation sets.\n",
    "   \n",
    "2. **Evaluate Performance**:\n",
    "   - Use cross-validation to evaluate the model's performance for different values of \\( K \\).\n",
    "\n",
    "3. **Plot Performance Metrics**:\n",
    "   - Plot metrics like accuracy (for classification) or mean squared error (for regression) against different values of \\( K \\).\n",
    "\n",
    "4. **Select Optimal \\( K \\)**:\n",
    "   - Choose the value of \\( K \\) that provides the best performance on the validation set.\n",
    "\n",
    "## Example Code (Python)\n",
    "Here's an example of how you might implement the process of choosing \\( K \\) using cross-validation in Python with scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "X = ...  # Features\n",
    "y = ...  # Target labels\n",
    "\n",
    "# Range of K values to try\n",
    "k_values = range(1, 31)\n",
    "cv_scores = []\n",
    "\n",
    "# Perform 10-fold cross-validation for each value of K\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Find the value of K that has the highest cross-validated accuracy\n",
    "optimal_k = k_values[np.argmax(cv_scores)]\n",
    "print(f'The optimal number of neighbors is {optimal_k}')\n",
    "\n",
    "# Plotting the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(k_values, cv_scores)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "plt.title('Choosing the Optimal K')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab20e39-4f90-47f2-b716-ff06568ba6c2",
   "metadata": {},
   "source": [
    "## Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5faeb6-b8ad-426c-bdaa-bd55ac2ffccb",
   "metadata": {},
   "source": [
    "# Difference Between KNN Classifier and KNN Regressor\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm can be used for both classification and regression tasks. Although the underlying mechanism is similar, the way predictions are made differs between the KNN classifier and KNN regressor.\n",
    "\n",
    "## KNN Classifier\n",
    "\n",
    "### Purpose\n",
    "- Used for classification tasks, where the goal is to assign a discrete class label to each input sample.\n",
    "\n",
    "### How It Works\n",
    "1. **Identify Neighbors**: For a given input sample, the algorithm identifies the `k` nearest neighbors from the training data based on a chosen distance metric (e.g., Euclidean distance).\n",
    "2. **Vote**: Each of the `k` neighbors \"votes\" for its class label.\n",
    "3. **Assign Class**: The class label with the majority vote among the `k` neighbors is assigned to the input sample.\n",
    "\n",
    "### Example\n",
    "- If `k=3` and the three nearest neighbors have class labels [0, 1, 1], the input sample will be assigned the class label `1` (since 1 appears more frequently).\n",
    "\n",
    "### Output\n",
    "- The output is a class label (categorical value).\n",
    "\n",
    "### Applications\n",
    "- Commonly used in image recognition, spam detection, and other classification problems.\n",
    "\n",
    "## KNN Regressor\n",
    "\n",
    "### Purpose\n",
    "- Used for regression tasks, where the goal is to predict a continuous numerical value for each input sample.\n",
    "\n",
    "### How It Works\n",
    "1. **Identify Neighbors**: For a given input sample, the algorithm identifies the `k` nearest neighbors from the training data based on a chosen distance metric.\n",
    "2. **Average**: The algorithm computes the average (or weighted average) of the target values of the `k` nearest neighbors.\n",
    "3. **Predict Value**: The computed average is assigned as the predicted value for the input sample.\n",
    "\n",
    "### Example\n",
    "- If `k=3` and the three nearest neighbors have target values [2.0, 3.5, 3.0], the predicted value for the input sample will be the average: (2.0 + 3.5 + 3.0) / 3 = 2.83.\n",
    "\n",
    "### Output\n",
    "- The output is a continuous numerical value.\n",
    "\n",
    "### Applications\n",
    "- Commonly used in predicting house prices, stock prices, and other regression problems.\n",
    "\n",
    "## Key Differences\n",
    "\n",
    "| Aspect               | KNN Classifier                                 | KNN Regressor                                |\n",
    "|----------------------|------------------------------------------------|---------------------------------------------|\n",
    "| Task                 | Classification                                 | Regression                                  |\n",
    "| Output               | Discrete class label (categorical)             | Continuous numerical value                  |\n",
    "| Decision Rule        | Majority vote among `k` nearest neighbors      | Average (or weighted average) of `k` nearest neighbors |\n",
    "| Example Application  | Image recognition, spam detection              | House price prediction, stock price forecasting |\n",
    "\n",
    "## Summary\n",
    "- **KNN Classifier**: Used for classification tasks where the output is a class label. It assigns the most common class among the `k` nearest neighbors to the input sample.\n",
    "- **KNN Regressor**: Used for regression tasks where the output is a continuous value. It predicts the value for the input sample by averaging the target values of the `k` nearest neighbors.\n",
    "\n",
    "Both versions of the KNN algorithm rely on the same core mechanism of identifying the nearest neighbors, but they differ in how they use these neighbors to make predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91faaeee-5df2-4a09-8e5a-8d301880f7cd",
   "metadata": {},
   "source": [
    "## Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6d957-ce88-45ec-b9b9-96838ea45c8d",
   "metadata": {},
   "source": [
    "# Measuring the Performance of KNN\n",
    "\n",
    "The performance of the K-Nearest Neighbors (KNN) algorithm can be measured using different metrics based on whether it is applied to classification or regression tasks.\n",
    "\n",
    "## Performance Metrics for KNN Classifier\n",
    "\n",
    "1. **Accuracy**:\n",
    "   - The ratio of correctly predicted instances to the total instances.\n",
    "   - Formula: \\( \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} \\)\n",
    "   - Suitable for balanced datasets.\n",
    "\n",
    "2. **Confusion Matrix**:\n",
    "   - A table that describes the performance of a classification model by showing the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
    "   - Useful for calculating other metrics.\n",
    "\n",
    "3. **Precision**:\n",
    "   - The ratio of correctly predicted positive observations to the total predicted positives.\n",
    "   - Formula: \\( \\text{Precision} = \\frac{TP}{TP + FP} \\)\n",
    "   - Indicates the accuracy of positive predictions.\n",
    "\n",
    "4. **Recall (Sensitivity or True Positive Rate)**:\n",
    "   - The ratio of correctly predicted positive observations to all observations in the actual class.\n",
    "   - Formula: \\( \\text{Recall} = \\frac{TP}{TP + FN} \\)\n",
    "   - Indicates the ability of the model to capture all positive instances.\n",
    "\n",
    "5. **F1 Score**:\n",
    "   - The harmonic mean of Precision and Recall.\n",
    "   - Formula: \\( \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\)\n",
    "   - Useful for imbalanced datasets.\n",
    "\n",
    "6. **ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**:\n",
    "   - AUC represents the degree or measure of separability between classes.\n",
    "   - ROC curve plots the True Positive Rate (Recall) against the False Positive Rate (1-Specificity).\n",
    "\n",
    "## Performance Metrics for KNN Regressor\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**:\n",
    "   - The average of the absolute differences between predicted and actual values.\n",
    "   - Formula: \\( \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i| \\)\n",
    "   - Indicates the average magnitude of errors in predictions.\n",
    "\n",
    "2. **Mean Squared Error (MSE)**:\n",
    "   - The average of the squared differences between predicted and actual values.\n",
    "   - Formula: \\( \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\)\n",
    "   - Penalizes larger errors more than MAE.\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE)**:\n",
    "   - The square root of the average squared differences between predicted and actual values.\n",
    "   - Formula: \\( \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2} \\)\n",
    "   - Provides a measure of the average magnitude of errors.\n",
    "\n",
    "4. **R-squared (Coefficient of Determination)**:\n",
    "   - The proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "   - Formula: \\( R^2 = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\)\n",
    "   - Indicates how well the model explains the variability of the target variable.\n",
    "\n",
    "## Example Code (Python)\n",
    "Hereâ€™s an example of how to compute these metrics using scikit-learn in Python:\n",
    "\n",
    "### For Classification\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Load dataset\n",
    "X, y = ...  # Features and target labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'Confusion Matrix: \\n{conf_matrix}')\n",
    "print(f'ROC-AUC: {roc_auc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74197dfb-9d22-4a13-93bf-d692b8015f3b",
   "metadata": {},
   "source": [
    "### FOR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5db3592-f704-43b9-a9ab-206ae056a651",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable ellipsis object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error, r2_score\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m  \u001b[38;5;66;03m# Features and target values\u001b[39;00m\n\u001b[0;32m      7\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Train KNN Regressor\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable ellipsis object"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "X, y = ...  # Features and target values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train KNN Regressor\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Compute metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'R-squared: {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b490e53-7f1d-4ff2-921e-f7b00e7eea95",
   "metadata": {},
   "source": [
    "## Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e09683-5c1c-4196-9ba6-c74d44bdd118",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality in KNN\n",
    "\n",
    "## Overview\n",
    "The curse of dimensionality is a term used to describe the difficulties and challenges that arise when working with high-dimensional data. In the context of the K-Nearest Neighbors (KNN) algorithm, it refers to the negative effects on the algorithm's performance as the number of features (dimensions) increases.\n",
    "\n",
    "## Key Issues\n",
    "\n",
    "1. **Increased Sparsity**:\n",
    "   - As the number of dimensions increases, the volume of the feature space grows exponentially, causing the data points to become sparser.\n",
    "   - In high-dimensional spaces, data points are spread out, making it difficult to find close neighbors.\n",
    "\n",
    "2. **Distance Metrics Lose Meaning**:\n",
    "   - In high-dimensional spaces, the differences in distances between the nearest and farthest neighbors tend to become negligible.\n",
    "   - The concept of \"nearness\" becomes less meaningful, and the effectiveness of distance-based algorithms like KNN diminishes.\n",
    "\n",
    "3. **Increased Computational Complexity**:\n",
    "   - The computational cost of calculating distances increases with the number of dimensions.\n",
    "   - High-dimensional datasets require significantly more computation to determine the nearest neighbors.\n",
    "\n",
    "## Effects on KNN\n",
    "\n",
    "1. **Degraded Performance**:\n",
    "   - KNN relies on the idea that similar data points are close to each other. In high-dimensional spaces, this assumption often breaks down.\n",
    "   - The algorithm may perform poorly as it becomes challenging to identify true nearest neighbors.\n",
    "\n",
    "2. **Overfitting Risk**:\n",
    "   - With a large number of dimensions, the model may become overly complex, capturing noise rather than the underlying pattern.\n",
    "   - High-dimensional data can lead to overfitting, where the model performs well on training data but poorly on new, unseen data.\n",
    "\n",
    "## Mitigation Strategies\n",
    "\n",
    "1. **Dimensionality Reduction**:\n",
    "   - Techniques such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) can reduce the number of dimensions while retaining most of the important information.\n",
    "   - Reducing dimensions helps to alleviate sparsity and makes the distance metrics more meaningful.\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - Selecting a subset of relevant features based on domain knowledge, statistical tests, or model-based approaches can help reduce dimensionality.\n",
    "   - Feature selection aims to keep the most informative features while discarding redundant or irrelevant ones.\n",
    "\n",
    "3. **Use of Advanced Algorithms**:\n",
    "   - Consider algorithms that are better suited for high-dimensional data, such as tree-based methods (e.g., Random Forests) or support vector machines with appropriate kernels.\n",
    "   - These algorithms may handle the complexities of high-dimensional spaces more effectively than KNN.\n",
    "\n",
    "## Example: Dimensionality Reduction with PCA\n",
    "\n",
    "Here's an example of applying PCA to reduce the dimensionality of a dataset before using KNN:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "X, y = ...  # Features and target labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply PCA to reduce dimensions\n",
    "pca = PCA(n_components=10)  # Reduce to 10 dimensions (for example)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train KNN on reduced data\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_pca, y_train)\n",
    "y_pred = knn.predict(X_test_pca)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy after PCA: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec74ce9-9761-46ea-9e74-ee3cedb6e669",
   "metadata": {},
   "source": [
    "## Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64288db3-b14f-4f51-af29-9186fb3595ad",
   "metadata": {},
   "source": [
    "# Handling Missing Values in KNN\n",
    "\n",
    "Missing values in a dataset can cause issues for the K-Nearest Neighbors (KNN) algorithm, as it relies on distance metrics to find neighbors. Here are several strategies to handle missing values effectively:\n",
    "\n",
    "## Strategies for Handling Missing Values\n",
    "\n",
    "1. **Remove Missing Values**:\n",
    "   - Simply remove rows or columns with missing values.\n",
    "   - This approach is straightforward but can lead to a significant loss of data, especially if the missing values are numerous.\n",
    "\n",
    "2. **Impute Missing Values**:\n",
    "   - Replace missing values with substituted values. Common imputation methods include:\n",
    "     - **Mean/Median Imputation**: Replace missing values with the mean or median of the column.\n",
    "     - **Mode Imputation**: Replace missing values with the mode (most frequent value) for categorical data.\n",
    "     - **KNN Imputation**: Use the KNN algorithm itself to impute missing values by finding the k-nearest neighbors and using their values.\n",
    "\n",
    "3. **Use Algorithms that Handle Missing Values**:\n",
    "   - Some algorithms can handle missing values natively (e.g., tree-based methods like Random Forests). However, this doesn't directly apply to KNN.\n",
    "\n",
    "### Mean/Median/Mode Imputation\n",
    "\n",
    "Simple and commonly used methods to handle missing values.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Example dataset\n",
    "data = {'feature1': [1, 2, None, 4, 5],\n",
    "        'feature2': [None, 1, 1, 1, 1],\n",
    "        'feature3': ['A', 'B', 'B', None, 'A']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mean imputation for numerical columns\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['feature1'] = imputer.fit_transform(df[['feature1']])\n",
    "\n",
    "# Mode imputation for categorical columns\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['feature3'] = imputer.fit_transform(df[['feature3']])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f181f310-abbf-48c1-b876-25d65aac8be0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature1  feature2  feature3\n",
      "0       1.0       1.0     0.500\n",
      "1       2.0       1.0     0.625\n",
      "2       3.0       1.0     0.750\n",
      "3       4.0       1.0     0.800\n",
      "4       5.0       1.0     1.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Example dataset\n",
    "data = {'feature1': [1, 2, None, 4, 5],\n",
    "        'feature2': [None, 1, 1, 1, 1],\n",
    "        'feature3': [0.5, None, 0.75, 0.8, 1.0]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# KNN Imputer\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f3e40f9-9e78-49be-846c-526eb3493322",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example dataset with target variable\n",
    "X = df_imputed\n",
    "y = [0, 1, 0, 1, 0]  # Example target values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c434d0c-4861-4a32-92bc-fd7b041173f6",
   "metadata": {},
   "source": [
    "## Considerations\n",
    "\n",
    "### Choice of Imputation Method\n",
    "- The choice of imputation method can affect the performance of the KNN algorithm. Simple imputation methods are fast but may not capture the underlying distribution of the data well.\n",
    "- KNN imputation considers the similarity between data points and can be more accurate, but it is computationally more expensive.\n",
    "\n",
    "### Consistency of Imputation\n",
    "- Ensure that the imputation method is applied consistently across training and test data to avoid data leakage.\n",
    "\n",
    "### Impact on Model Performance\n",
    "- Always evaluate the impact of the chosen imputation method on the performance of your KNN model. Cross-validation can help in assessing this impact.\n",
    "\n",
    "By handling missing values appropriately, you can improve the reliability and performance of the KNN algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f4b12-66f5-49b4-8c3c-1554b8fa3ee6",
   "metadata": {},
   "source": [
    "## Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c93a3-92dd-4121-8f07-5a3b6785505a",
   "metadata": {},
   "source": [
    "# Comparing KNN Classifier and KNN Regressor\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a versatile algorithm that can be used for both classification and regression tasks. While the underlying principle of finding the nearest neighbors remains the same, the application and performance of KNN differ based on the type of problem.\n",
    "\n",
    "## KNN Classifier\n",
    "\n",
    "### Overview\n",
    "- **Purpose**: Used for classification tasks where the goal is to assign a class label to an instance based on the class labels of its nearest neighbors.\n",
    "- **Output**: Class label (categorical value).\n",
    "\n",
    "### Performance\n",
    "- **Strengths**:\n",
    "  - **Simplicity**: Easy to understand and implement.\n",
    "  - **Flexibility**: Can handle multi-class classification problems.\n",
    "  - **Non-parametric**: No assumptions about the underlying data distribution.\n",
    "\n",
    "- **Weaknesses**:\n",
    "  - **Computational Complexity**: High computational cost for large datasets due to distance calculations.\n",
    "  - **Curse of Dimensionality**: Performance degrades with an increase in the number of features.\n",
    "  - **Sensitivity to Irrelevant Features**: Can be affected by irrelevant or redundant features.\n",
    "\n",
    "### Best Use Cases\n",
    "- Problems with a relatively small number of features.\n",
    "- When the decision boundaries are not linear.\n",
    "- Situations where interpretability is important.\n",
    "\n",
    "### Example Metrics\n",
    "- Accuracy, Precision, Recall, F1 Score, ROC-AUC.\n",
    "\n",
    "## KNN Regressor\n",
    "\n",
    "### Overview\n",
    "- **Purpose**: Used for regression tasks where the goal is to predict a continuous value based on the values of its nearest neighbors.\n",
    "- **Output**: Continuous value.\n",
    "\n",
    "### Performance\n",
    "- **Strengths**:\n",
    "  - **Simplicity**: Easy to understand and implement.\n",
    "  - **Non-parametric**: No assumptions about the underlying data distribution.\n",
    "  - **Flexibility**: Can model non-linear relationships.\n",
    "\n",
    "- **Weaknesses**:\n",
    "  - **Computational Complexity**: High computational cost for large datasets due to distance calculations.\n",
    "  - **Curse of Dimensionality**: Performance degrades with an increase in the number of features.\n",
    "  - **Sensitivity to Outliers**: Predictions can be heavily influenced by outliers.\n",
    "\n",
    "### Best Use Cases\n",
    "- Problems where the relationship between features and the target is non-linear.\n",
    "- When interpretability and simplicity are important.\n",
    "- Situations with a moderate number of features and instances.\n",
    "\n",
    "### Example Metrics\n",
    "- Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared.\n",
    "\n",
    "## Comparison\n",
    "\n",
    "### Similarities\n",
    "- Both rely on the principle of finding the k-nearest neighbors.\n",
    "- Both are non-parametric and lazy learners (do not learn an explicit model during training).\n",
    "- Both are sensitive to the choice of distance metric and the value of k.\n",
    "\n",
    "### Differences\n",
    "- **Output**:\n",
    "  - Classifier: Predicts categorical labels.\n",
    "  - Regressor: Predicts continuous values.\n",
    "- **Performance Metrics**:\n",
    "  - Classifier: Evaluated using classification metrics like accuracy, precision, recall, etc.\n",
    "  - Regressor: Evaluated using regression metrics like MAE, MSE, RMSE, and R-squared.\n",
    "- **Use Cases**:\n",
    "  - Classifier: Suitable for tasks where the target variable is categorical (e.g., spam detection, image classification).\n",
    "  - Regressor: Suitable for tasks where the target variable is continuous (e.g., predicting house prices, stock prices).\n",
    "\n",
    "## Conclusion\n",
    "- **KNN Classifier** is better suited for classification problems where the goal is to categorize instances into discrete classes.\n",
    "- **KNN Regressor** is better suited for regression problems where the goal is to predict a continuous value.\n",
    "\n",
    "By understanding the strengths and weaknesses of both the KNN classifier and regressor, you can choose the appropriate variant for your specific problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f89ad9-9a1d-4e68-9457-8e0966372529",
   "metadata": {},
   "source": [
    "## Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02cb8d-56a0-45c1-a6ca-7435314389c4",
   "metadata": {},
   "source": [
    "# Strengths and Weaknesses of the KNN Algorithm\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple yet powerful algorithm for both classification and regression tasks. However, it has its own set of strengths and weaknesses.\n",
    "\n",
    "## Strengths\n",
    "\n",
    "1. **Simplicity**:\n",
    "   - **Easy to Understand and Implement**: KNN is intuitive and straightforward to implement.\n",
    "   - **No Training Phase**: As a lazy learner, KNN does not require a training phase, making it easy to set up.\n",
    "\n",
    "2. **Flexibility**:\n",
    "   - **Non-parametric**: KNN makes no assumptions about the underlying data distribution, allowing it to model complex, non-linear relationships.\n",
    "   - **Versatile**: Can be used for both classification and regression problems.\n",
    "\n",
    "3. **Effectiveness with Small Datasets**:\n",
    "   - Performs well with small to moderately sized datasets where computational complexity is not an issue.\n",
    "\n",
    "## Weaknesses\n",
    "\n",
    "1. **Computational Complexity**:\n",
    "   - **High Computational Cost**: Distance calculations for all points in the dataset can be computationally expensive, especially with large datasets.\n",
    "   - **Slow Predictions**: As a lazy learner, KNN can be slow during prediction time because it processes all data points to find the nearest neighbors.\n",
    "\n",
    "2. **Curse of Dimensionality**:\n",
    "   - **Performance Degradation**: As the number of features increases, the distance between points becomes less meaningful, leading to degraded performance.\n",
    "   - **Data Sparsity**: High-dimensional spaces can cause data points to become sparse, making it difficult to find meaningful neighbors.\n",
    "\n",
    "3. **Sensitivity to Irrelevant Features and Noise**:\n",
    "   - **Irrelevant Features**: KNN is sensitive to irrelevant or redundant features, which can affect its performance.\n",
    "   - **Noise**: Outliers and noisy data points can significantly impact the predictions.\n",
    "\n",
    "4. **Choice of Distance Metric and Hyperparameters**:\n",
    "   - **Distance Metric**: The choice of distance metric (e.g., Euclidean, Manhattan) can affect the algorithm's performance.\n",
    "   - **Value of K**: Selecting the appropriate number of neighbors (k) is crucial and can be challenging.\n",
    "\n",
    "## Addressing the Weaknesses\n",
    "\n",
    "1. **Improving Computational Efficiency**:\n",
    "   - **KD-Trees and Ball Trees**: Use data structures like KD-Trees or Ball Trees to speed up the nearest neighbor search.\n",
    "   - **Approximate Nearest Neighbors**: Use algorithms that approximate the nearest neighbors to reduce computational complexity.\n",
    "\n",
    "2. **Mitigating the Curse of Dimensionality**:\n",
    "   - **Dimensionality Reduction**: Apply techniques like Principal Component Analysis (PCA) or t-SNE to reduce the number of features.\n",
    "   - **Feature Selection**: Select the most relevant features using statistical tests or model-based approaches.\n",
    "\n",
    "3. **Handling Irrelevant Features and Noise**:\n",
    "   - **Normalization/Standardization**: Scale features to ensure they contribute equally to distance calculations.\n",
    "   - **Outlier Detection and Removal**: Identify and remove outliers before applying KNN.\n",
    "   - **Feature Engineering**: Carefully engineer features to include only relevant information.\n",
    "\n",
    "4. **Optimizing Distance Metric and Hyperparameters**:\n",
    "   - **Hyperparameter Tuning**: Use cross-validation techniques to find the optimal value of k.\n",
    "   - **Distance Metric Selection**: Experiment with different distance metrics and choose the one that performs best for your data.\n",
    "\n",
    "## Example: Addressing Weaknesses with Preprocessing and Hyperparameter Tuning\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "X, y = ...  # Features and target labels\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid = {'n_neighbors': range(1, 21), 'metric': ['euclidean', 'manhattan']}\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# Best model\n",
    "best_knn = grid_search.best_estimator_\n",
    "y_pred = best_knn.predict(X_test_pca)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe76b526-193d-44a0-92dd-6b0fffc42863",
   "metadata": {},
   "source": [
    "## Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e6026-ac5c-43b2-a875-8a18d2545a0c",
   "metadata": {},
   "source": [
    "# Difference Between Euclidean Distance and Manhattan Distance in KNN\n",
    "\n",
    "K-Nearest Neighbors (KNN) relies on distance metrics to determine the similarity between data points. Two commonly used distance metrics are Euclidean distance and Manhattan distance. Here, we explore their differences and implications for KNN.\n",
    "\n",
    "## Euclidean Distance\n",
    "\n",
    "### Definition\n",
    "- Euclidean distance is the straight-line distance between two points in Euclidean space.\n",
    "- It is calculated using the Pythagorean theorem.\n",
    "\n",
    "### Formula\n",
    "For two points \\( p = (p_1, p_2, \\ldots, p_n) \\) and \\( q = (q_1, q_2, \\ldots, q_n) \\) in n-dimensional space, the Euclidean distance \\( d(p, q) \\) is given by:\n",
    "\\[ d(p, q) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2} \\]\n",
    "\n",
    "### Characteristics\n",
    "- **Sensitive to Magnitude**: Larger differences in feature values contribute more to the distance.\n",
    "- **Metric Properties**: Euclidean distance satisfies all properties of a metric (non-negativity, identity of indiscernibles, symmetry, and triangle inequality).\n",
    "\n",
    "### Use Cases\n",
    "- Suitable for continuous, real-valued data.\n",
    "- Often used when the relationship between features is more linear.\n",
    "\n",
    "### Example\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "point1 = np.array([1, 2])\n",
    "point2 = np.array([4, 6])\n",
    "euclidean_distance = np.linalg.norm(point1 - point2)\n",
    "print(f'Euclidean Distance: {euclidean_distance}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f27e04a-a383-4ae5-9dff-850b25431efd",
   "metadata": {},
   "source": [
    "## Manhattan Distance\n",
    "\n",
    "### Definition\n",
    "- Manhattan distance, also known as L1 distance or city block distance, is the sum of the absolute differences between the coordinates of two points.\n",
    "- It represents the distance one would travel in a grid-like path (like streets in a city).\n",
    "\n",
    "### Formula\n",
    "For two points \\( p = (p_1, p_2, \\ldots, p_n) \\) and \\( q = (q_1, q_2, \\ldots, q_n) \\) in n-dimensional space, the Manhattan distance \\( d(p, q) \\) is given by:\n",
    "\\[ d(p, q) = \\sum_{i=1}^{n} |p_i - q_i| \\]\n",
    "\n",
    "### Characteristics\n",
    "- **Less Sensitive to Outliers**: Outliers have a less exaggerated effect compared to Euclidean distance.\n",
    "- **Metric Properties**: Manhattan distance also satisfies all properties of a metric.\n",
    "\n",
    "### Use Cases\n",
    "- Suitable for categorical data or data with distinct features.\n",
    "- Often used in high-dimensional spaces where coordinates can be seen as discrete steps.\n",
    "\n",
    "### Example\n",
    "```python\n",
    "point1 = np.array([1, 2])\n",
    "point2 = np.array([4, 6])\n",
    "manhattan_distance = np.sum(np.abs(point1 - point2))\n",
    "print(f'Manhattan Distance: {manhattan_distance}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed44cf-363d-455c-a2b5-21b3d9294acc",
   "metadata": {},
   "source": [
    "## Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac015062-176c-4d98-a075-a988b41c469b",
   "metadata": {},
   "source": [
    "# Role of Feature Scaling in KNN\n",
    "\n",
    "Feature scaling, also known as normalization or standardization, plays a crucial role in the K-Nearest Neighbors (KNN) algorithm. KNN relies on distance metrics to determine the similarity between data points. Therefore, the scale and magnitude of features can significantly impact the performance of the algorithm.\n",
    "\n",
    "## Importance of Feature Scaling\n",
    "\n",
    "1. **Equalizing Feature Contributions**:\n",
    "   - Features with larger scales can dominate the distance calculations, leading to biased results.\n",
    "   - Feature scaling ensures that all features contribute equally to the distance metric, preventing any single feature from having a disproportionate influence.\n",
    "\n",
    "2. **Improving Convergence**:\n",
    "   - Feature scaling can help algorithms converge more quickly, especially in optimization-based methods or distance-based algorithms like KNN.\n",
    "   - It can lead to faster and more stable convergence by making the objective function or distance metric more isotropic (symmetric in all directions).\n",
    "\n",
    "3. **Avoiding Numerical Instabilities**:\n",
    "   - Large differences in feature scales can lead to numerical instabilities, especially in algorithms that involve matrix inversion or optimization.\n",
    "   - Feature scaling mitigates these instabilities and improves the numerical robustness of the algorithm.\n",
    "\n",
    "## Methods of Feature Scaling\n",
    "\n",
    "1. **Normalization (Min-Max Scaling)**:\n",
    "   - Rescales the feature values to a range between 0 and 1.\n",
    "   - Formula: \\( X_{\\text{normalized}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\)\n",
    "\n",
    "2. **Standardization (Z-Score Scaling)**:\n",
    "   - Standardizes the feature values to have a mean of 0 and a standard deviation of 1.\n",
    "   - Formula: \\( X_{\\text{standardized}} = \\frac{X - \\mu}{\\sigma} \\), where \\( \\mu \\) is the mean and \\( \\sigma \\) is the standard deviation of the feature.\n",
    "\n",
    "3. **Robust Scaling**:\n",
    "   - Scales the feature values based on robust estimators like the median and interquartile range (IQR).\n",
    "   - More resilient to outliers compared to Min-Max scaling and Standardization.\n",
    "\n",
    "## Example\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Example dataset\n",
    "X = ...  # Features\n",
    "y = ...  # Target labels\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler to features and transform\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Instantiate KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train classifier with scaled features\n",
    "knn.fit(X_scaled, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd28a1b-f6c9-46e6-b59a-00d64665ef54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
