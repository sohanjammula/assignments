{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cfdce5f-dc46-4738-961a-b50f32092ed4",
   "metadata": {},
   "source": [
    "### Q10. Explain the concept of batch normalization in the context of Artificial Neural Networks.\n",
    "\n",
    "#### Batch Normalization (BatchNorm):\n",
    "\n",
    "Batch normalization is a technique used in artificial neural networks to standardize the inputs of each layer by adjusting and scaling the activations. It aims to address the issue of internal covariate shift, which occurs when the distribution of inputs to a layer changes during training, leading to slower convergence and unstable gradients.\n",
    "\n",
    "### Q11. Describe the benefits of using batch normalization during training.\n",
    "\n",
    "#### Benefits of Batch Normalization:\n",
    "\n",
    "1. **Improved Training Speed:**\n",
    "   - Batch normalization helps accelerate the training process by reducing the effects of internal covariate shift.\n",
    "   - It stabilizes and speeds up convergence, allowing neural networks to converge faster and achieve higher accuracy in fewer training iterations.\n",
    "\n",
    "2. **Better Gradient Flow:**\n",
    "   - Batch normalization smoothens the optimization landscape by normalizing the inputs of each layer, which results in more consistent gradients during backpropagation.\n",
    "   - This leads to improved gradient flow, mitigating issues such as vanishing or exploding gradients, especially in deeper networks.\n",
    "\n",
    "3. **Regularization Effect:**\n",
    "   - Batch normalization acts as a form of regularization by adding noise to the activations during training.\n",
    "   - This noise helps prevent overfitting and improves the generalization performance of the model, leading to better performance on unseen data.\n",
    "\n",
    "4. **Reduced Sensitivity to Initialization:**\n",
    "   - Batch normalization reduces the sensitivity of neural networks to weight initialization choices.\n",
    "   - It allows for more aggressive weight initialization strategies, such as Xavier or He initialization, without causing convergence issues or affecting the model's performance.\n",
    "\n",
    "### Q12. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters.\n",
    "\n",
    "#### Working Principle of Batch Normalization:\n",
    "\n",
    "1. **Normalization Step:**\n",
    "   - In batch normalization, the activations of each layer are normalized using the mean and standard deviation computed over a mini-batch of samples.\n",
    "   - The normalized activations are then scaled and shifted using learnable parameters (gamma and beta) to allow the network to learn the optimal representation for each layer.\n",
    "\n",
    "2. **Learnable Parameters (Gamma and Beta):**\n",
    "   - Batch normalization introduces two learnable parameters per feature map/channel: gamma (scaling factor) and beta (shift parameter).\n",
    "   - Gamma and beta allow the network to learn the optimal scaling and shifting of the normalized activations, enabling the model to adapt to different distributions of input data and capture complex patterns effectively.\n",
    "\n",
    "3. **Normalization Across Samples:**\n",
    "   - During training, batch normalization normalizes the activations across the mini-batch dimension, ensuring that each layer receives inputs with a consistent distribution.\n",
    "   - This normalization step helps stabilize training by reducing the internal covariate shift and makes the network more robust to changes in input distributions.\n",
    "\n",
    "In summary, batch normalization standardizes the inputs of each layer by normalizing activations using mini-batch statistics and learns optimal scaling and shifting parameters to improve training speed, gradient flow, regularization, and robustness of neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6d08a7e-27e3-418e-b794-cbc91ab09de2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [01:15<00:00, 130693.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 88053.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1000854.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 546941.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load MNIST train and test datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae880cdd-4a84-41fc-872e-8c552a444ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network architecture\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the input images\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model_no_bn = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_no_bn.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eec0a39-2339-4ec4-b398-16865da8fb74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/938], Loss: 1.0328\n",
      "Epoch [1/5], Step [200/938], Loss: 0.4491\n",
      "Epoch [1/5], Step [300/938], Loss: 0.3728\n",
      "Epoch [1/5], Step [400/938], Loss: 0.3341\n",
      "Epoch [1/5], Step [500/938], Loss: 0.3153\n",
      "Epoch [1/5], Step [600/938], Loss: 0.3161\n",
      "Epoch [1/5], Step [700/938], Loss: 0.2833\n",
      "Epoch [1/5], Step [800/938], Loss: 0.2753\n",
      "Epoch [1/5], Step [900/938], Loss: 0.2598\n",
      "Epoch [2/5], Step [100/938], Loss: 0.2348\n",
      "Epoch [2/5], Step [200/938], Loss: 0.2135\n",
      "Epoch [2/5], Step [300/938], Loss: 0.2275\n",
      "Epoch [2/5], Step [400/938], Loss: 0.1979\n",
      "Epoch [2/5], Step [500/938], Loss: 0.1823\n",
      "Epoch [2/5], Step [600/938], Loss: 0.1903\n",
      "Epoch [2/5], Step [700/938], Loss: 0.1749\n",
      "Epoch [2/5], Step [800/938], Loss: 0.1657\n",
      "Epoch [2/5], Step [900/938], Loss: 0.1753\n",
      "Epoch [3/5], Step [100/938], Loss: 0.1552\n",
      "Epoch [3/5], Step [200/938], Loss: 0.1371\n",
      "Epoch [3/5], Step [300/938], Loss: 0.1592\n",
      "Epoch [3/5], Step [400/938], Loss: 0.1382\n",
      "Epoch [3/5], Step [500/938], Loss: 0.1300\n",
      "Epoch [3/5], Step [600/938], Loss: 0.1278\n",
      "Epoch [3/5], Step [700/938], Loss: 0.1520\n",
      "Epoch [3/5], Step [800/938], Loss: 0.1289\n",
      "Epoch [3/5], Step [900/938], Loss: 0.1314\n",
      "Epoch [4/5], Step [100/938], Loss: 0.1127\n",
      "Epoch [4/5], Step [200/938], Loss: 0.0971\n",
      "Epoch [4/5], Step [300/938], Loss: 0.1044\n",
      "Epoch [4/5], Step [400/938], Loss: 0.1173\n",
      "Epoch [4/5], Step [500/938], Loss: 0.1244\n",
      "Epoch [4/5], Step [600/938], Loss: 0.1177\n",
      "Epoch [4/5], Step [700/938], Loss: 0.1201\n",
      "Epoch [4/5], Step [800/938], Loss: 0.1149\n",
      "Epoch [4/5], Step [900/938], Loss: 0.1061\n",
      "Epoch [5/5], Step [100/938], Loss: 0.0865\n",
      "Epoch [5/5], Step [200/938], Loss: 0.0999\n",
      "Epoch [5/5], Step [300/938], Loss: 0.0934\n",
      "Epoch [5/5], Step [400/938], Loss: 0.0817\n",
      "Epoch [5/5], Step [500/938], Loss: 0.0965\n",
      "Epoch [5/5], Step [600/938], Loss: 0.0959\n",
      "Epoch [5/5], Step [700/938], Loss: 0.0956\n",
      "Epoch [5/5], Step [800/938], Loss: 0.1005\n",
      "Epoch [5/5], Step [900/938], Loss: 0.1052\n"
     ]
    }
   ],
   "source": [
    "# Define training function\n",
    "def train(model, criterion, optimizer, train_loader, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "# Train the model without batch normalization\n",
    "train(model_no_bn, criterion, optimizer, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8e7badf-63e7-49ee-9167-e85a8262c20e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the neural network architecture with batch normalization\n",
    "class SimpleNN_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN_BN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model with batch normalization, loss function, and optimizer\n",
    "model_bn = SimpleNN_BN()\n",
    "optimizer_bn = optim.Adam(model_bn.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "761646da-ac46-4a0e-9932-07f4d06418e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/938], Loss: 0.8243\n",
      "Epoch [1/5], Step [200/938], Loss: 0.3350\n",
      "Epoch [1/5], Step [300/938], Loss: 0.2445\n",
      "Epoch [1/5], Step [400/938], Loss: 0.1906\n",
      "Epoch [1/5], Step [500/938], Loss: 0.1723\n",
      "Epoch [1/5], Step [600/938], Loss: 0.1663\n",
      "Epoch [1/5], Step [700/938], Loss: 0.1372\n",
      "Epoch [1/5], Step [800/938], Loss: 0.1449\n",
      "Epoch [1/5], Step [900/938], Loss: 0.1344\n",
      "Epoch [2/5], Step [100/938], Loss: 0.0971\n",
      "Epoch [2/5], Step [200/938], Loss: 0.0853\n",
      "Epoch [2/5], Step [300/938], Loss: 0.1057\n",
      "Epoch [2/5], Step [400/938], Loss: 0.0957\n",
      "Epoch [2/5], Step [500/938], Loss: 0.1001\n",
      "Epoch [2/5], Step [600/938], Loss: 0.0950\n",
      "Epoch [2/5], Step [700/938], Loss: 0.0913\n",
      "Epoch [2/5], Step [800/938], Loss: 0.0946\n",
      "Epoch [2/5], Step [900/938], Loss: 0.0918\n",
      "Epoch [3/5], Step [100/938], Loss: 0.0585\n",
      "Epoch [3/5], Step [200/938], Loss: 0.0717\n",
      "Epoch [3/5], Step [300/938], Loss: 0.0620\n",
      "Epoch [3/5], Step [400/938], Loss: 0.0577\n",
      "Epoch [3/5], Step [500/938], Loss: 0.0744\n",
      "Epoch [3/5], Step [600/938], Loss: 0.0766\n",
      "Epoch [3/5], Step [700/938], Loss: 0.0742\n",
      "Epoch [3/5], Step [800/938], Loss: 0.0692\n",
      "Epoch [3/5], Step [900/938], Loss: 0.0610\n",
      "Epoch [4/5], Step [100/938], Loss: 0.0536\n",
      "Epoch [4/5], Step [200/938], Loss: 0.0529\n",
      "Epoch [4/5], Step [300/938], Loss: 0.0546\n",
      "Epoch [4/5], Step [400/938], Loss: 0.0464\n",
      "Epoch [4/5], Step [500/938], Loss: 0.0473\n",
      "Epoch [4/5], Step [600/938], Loss: 0.0537\n",
      "Epoch [4/5], Step [700/938], Loss: 0.0583\n",
      "Epoch [4/5], Step [800/938], Loss: 0.0556\n",
      "Epoch [4/5], Step [900/938], Loss: 0.0532\n",
      "Epoch [5/5], Step [100/938], Loss: 0.0429\n",
      "Epoch [5/5], Step [200/938], Loss: 0.0443\n",
      "Epoch [5/5], Step [300/938], Loss: 0.0344\n",
      "Epoch [5/5], Step [400/938], Loss: 0.0358\n",
      "Epoch [5/5], Step [500/938], Loss: 0.0429\n",
      "Epoch [5/5], Step [600/938], Loss: 0.0461\n",
      "Epoch [5/5], Step [700/938], Loss: 0.0413\n",
      "Epoch [5/5], Step [800/938], Loss: 0.0558\n",
      "Epoch [5/5], Step [900/938], Loss: 0.0460\n"
     ]
    }
   ],
   "source": [
    "# Train the model with batch normalization\n",
    "train(model_bn, criterion, optimizer_bn, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5349463b-cf12-4dc5-9d6e-8f101c55685b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without BatchNorm: 0.9667\n",
      "Accuracy with BatchNorm: 0.9751\n"
     ]
    }
   ],
   "source": [
    "# Define a function to evaluate model performance\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate models on the test dataset\n",
    "accuracy_no_bn = evaluate_model(model_no_bn, test_loader)\n",
    "accuracy_bn = evaluate_model(model_bn, test_loader)\n",
    "\n",
    "print(f'Accuracy without BatchNorm: {accuracy_no_bn}')\n",
    "print(f'Accuracy with BatchNorm: {accuracy_bn}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89b96f-db37-4cbd-838b-1adb229404e8",
   "metadata": {},
   "source": [
    "7. Discuss the impact of batch normalization on the training process and the performance of the neural network:\n",
    "Batch normalization improves the convergence speed and stability of the training process by normalizing the activations within each layer. It helps alleviate issues such as internal covariate shift, vanishing gradients, and overfitting. In this comparison, you may observe that the model with batch normalization achieves higher accuracy on the test dataset due to better regularization and improved gradient flow during training. Additionally, batch normalization reduces the sensitivity of the model to hyperparameters and initialization choices, making the training process more robust and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffc4270-953f-42d0-aed0-c9475e45c57a",
   "metadata": {},
   "source": [
    "## Experimentation and Analysis\n",
    "\n",
    "Experiment with different batch sizes and observe the effect on the training dynamics and model performance.\n",
    "\n",
    "**Discussion Points:**\n",
    "\n",
    "### Training Dynamics\n",
    "- **Smaller Batch Sizes**: \n",
    "    - Faster convergence initially due to more frequent parameter updates.\n",
    "    - Potential for noisy gradients, affecting convergence stability.\n",
    "- **Larger Batch Sizes**:\n",
    "    - Slower convergence but smoother optimization trajectories.\n",
    "    - More stable gradients but fewer updates per epoch.\n",
    "\n",
    "### Generalization\n",
    "- Smaller batch sizes may aid in better generalization by introducing randomness.\n",
    "- Larger batch sizes might offer smoother optimization, potentially aiding generalization.\n",
    "\n",
    "### Resource Utilization\n",
    "- Larger batch sizes maximize hardware resources like GPUs.\n",
    "- Smaller batch sizes may underutilize resources due to processing overhead.\n",
    "\n",
    "### Model Performance\n",
    "- Finding a balance between batch size and model performance is crucial.\n",
    "- Optimal batch size varies depending on the dataset and network architecture.\n",
    "\n",
    "## Batch Normalization: Advantages and Limitations\n",
    "\n",
    "### Advantages:\n",
    "1. **Faster Convergence**: \n",
    "    - Reduces internal covariate shift, accelerating training.\n",
    "2. **Regularization**: \n",
    "    - Acts as a form of regularization, aiding in preventing overfitting.\n",
    "3. **Stable Gradients**: \n",
    "    - Maintains stable gradients, reducing vanishing/exploding gradients.\n",
    "4. **Robustness to Initialization**: \n",
    "    - Reduces sensitivity to weight initialization.\n",
    "\n",
    "### Limitations:\n",
    "1. **Batch Size Sensitivity**: \n",
    "    - Effectiveness may vary with batch size choice.\n",
    "2. **Test-time Dependency**: \n",
    "    - Requires batch statistics from training during inference.\n",
    "3. **Increased Memory Usage**: \n",
    "    - Requires additional memory for storing statistics.\n",
    "4. **Computational Overhead**: \n",
    "    - Adds computational overhead to forward and backward passes.\n",
    "\n",
    "In conclusion, while batch normalization is powerful, its effectiveness and limitations should be carefully considered, especially concerning batch size selection and deployment scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad97b3-0445-4803-8b5c-f980ddd05ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
