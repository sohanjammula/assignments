{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d3e482-1134-4c29-be47-4f7b3bfb0403",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8fc32-f698-4bdb-9d33-f2e4dd0dab87",
   "metadata": {},
   "source": [
    "\n",
    "Lasso Regression, short for Least Absolute Shrinkage and Selection Operator Regression, is a type of linear regression that adds a regularization term to the ordinary least squares (OLS) objective function. It differs from other regression techniques, such as Ridge Regression or ordinary least squares regression, primarily in the type of regularization it applies and the impact it has on the resulting model.\n",
    "\n",
    "Here's an overview of Lasso Regression and its key differences from other regression techniques:\n",
    "\n",
    "Regularization Technique:\n",
    "Lasso Regression adds an L1 penalty term to the OLS objective function, in contrast to Ridge Regression, which adds an L2 penalty term.\n",
    "The L1 penalty term in Lasso Regression is the sum of the absolute values of the coefficients, whereas the L2 penalty term in Ridge Regression is the sum of the squared coefficients.\n",
    "The L1 penalty in Lasso Regression has a sparsity-inducing property, leading to some coefficients being exactly zero, effectively performing variable selection.\n",
    "Sparsity:\n",
    "One of the key features of Lasso Regression is its ability to produce sparse solutions by setting some coefficients exactly to zero.\n",
    "This sparsity-inducing property makes Lasso Regression well-suited for feature selection, as it automatically identifies and selects the most important predictors while effectively removing irrelevant predictors from the model.\n",
    "In contrast, Ridge Regression tends to shrink coefficients towards zero without setting them exactly to zero, leading to non-sparse solutions.\n",
    "Bias-Variance Trade-off:\n",
    "Lasso Regression introduces a bias by forcing some coefficients to zero, which can lead to an increase in bias compared to ordinary least squares regression.\n",
    "However, this bias can be beneficial in reducing variance and preventing overfitting, especially in high-dimensional datasets with many predictors.\n",
    "Interpretability:\n",
    "Lasso Regression can lead to more interpretable models by automatically performing feature selection and producing sparse solutions.\n",
    "The resulting model may contain fewer predictors, making it easier to understand and interpret the relationships between predictors and the outcome variable.\n",
    "In contrast, Ridge Regression tends to retain all predictors in the model, which may make interpretation more challenging, especially when dealing with a large number of predictors.\n",
    "In summary, Lasso Regression differs from other regression techniques, such as Ridge Regression or ordinary least squares regression, primarily in its sparsity-inducing property, which allows it to perform automatic feature selection by setting some coefficients to zero. This makes Lasso Regression particularly useful when dealing with high-dimensional datasets or when interpretability and feature selection are important considerations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b16961-1327-49f7-991b-eb1fe9607a56",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e927226-6818-4aba-84c6-246829e58fc0",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression in feature selection is its ability to automatically identify and select the most relevant predictors while effectively removing irrelevant predictors from the model. This sparsity-inducing property of Lasso Regression is highly beneficial in high-dimensional datasets or situations where feature selection is crucial. Here's a more detailed explanation of the advantages:\n",
    "\n",
    "Automatic Feature Selection:\n",
    "Lasso Regression automatically selects a subset of predictors by setting some coefficients to exactly zero. This leads to a sparse solution where only the most relevant predictors are retained in the model.\n",
    "By effectively removing irrelevant predictors, Lasso Regression helps simplify the model and improves its interpretability.\n",
    "Reduced Overfitting:\n",
    "The sparsity-inducing property of Lasso Regression helps prevent overfitting by reducing the complexity of the model.\n",
    "Overfitting occurs when the model learns to fit the noise in the training data rather than capturing the underlying patterns. By selecting only the most important predictors, Lasso Regression reduces the risk of overfitting and improves the model's generalization performance on unseen data.\n",
    "Improved Interpretability:\n",
    "The resulting model from Lasso Regression is often more interpretable because it contains fewer predictors.\n",
    "With fewer predictors to consider, it becomes easier to understand the relationships between predictors and the outcome variable.\n",
    "This is particularly advantageous in fields where interpretability is essential, such as healthcare or finance, where decision-making based on the model's predictions requires clear understanding of the underlying factors.\n",
    "Handling Multicollinearity:\n",
    "Lasso Regression can effectively handle multicollinearity, where predictors are highly correlated with each other.\n",
    "By selecting only one predictor from a group of highly correlated predictors, Lasso Regression helps alleviate multicollinearity issues and improves the stability of the coefficient estimates.\n",
    "Flexibility in Model Complexity:\n",
    "Lasso Regression provides flexibility in controlling the complexity of the model through the regularization parameter (\n",
    "ùúÜ\n",
    "Œª).\n",
    "By tuning \n",
    "ùúÜ\n",
    "Œª, users can balance between the degree of sparsity (number of selected predictors) and the model's predictive performance, allowing for customization based on the specific requirements of the problem.\n",
    "In summary, the main advantage of using Lasso Regression in feature selection is its ability to automatically select the most relevant predictors, reduce overfitting, improve interpretability, handle multicollinearity, and provide flexibility in controlling model complexity. These advantages make Lasso Regression a powerful tool in machine learning and statistical modeling, particularly in scenarios with high-dimensional data or when interpretability and feature selection are important considerations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd9adc8-9009-4df1-bd8d-99668be1d53a",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef83a8f-1012-442d-8b7c-a05eca989195",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of a Lasso Regression model involves understanding how changes in the predictor variables impact the outcome variable while considering the effects of regularization. Here's how you can interpret the coefficients of a Lasso Regression model:\n",
    "\n",
    "Magnitude:\n",
    "The magnitude of a coefficient represents the strength of the relationship between the corresponding predictor variable and the outcome variable.\n",
    "Larger coefficient magnitudes indicate stronger influences of the predictors on the outcome.\n",
    "Direction:\n",
    "The sign of a coefficient (+ or -) indicates the direction of the relationship between the predictor variable and the outcome variable.\n",
    "A positive coefficient indicates a positive relationship, meaning that an increase in the predictor variable is associated with an increase in the outcome variable, and vice versa for a negative coefficient.\n",
    "Sparsity:\n",
    "One of the key features of Lasso Regression is its ability to produce sparse solutions by setting some coefficients exactly to zero.\n",
    "Coefficients that are set to zero indicate that the corresponding predictors have been excluded from the model and do not contribute to the prediction.\n",
    "Non-zero coefficients indicate the predictors that are included in the model and have a non-negligible impact on the outcome variable.\n",
    "Regularization Effect:\n",
    "In Lasso Regression, the coefficients are shrunk towards zero due to the L1 penalty term, which encourages sparsity.\n",
    "As a result, some coefficients may be smaller in magnitude compared to ordinary least squares (OLS) regression or other regression techniques, where no regularization is applied.\n",
    "Comparison Across Models:\n",
    "When comparing coefficients across different models or different regularization strengths, it's essential to consider the regularization effect.\n",
    "Stronger regularization (higher \n",
    "ùúÜ\n",
    "Œª) leads to more shrinkage of coefficients, resulting in smaller magnitude coefficients compared to weaker regularization.\n",
    "Interpretation with Scaling:\n",
    "The interpretation of coefficients may also depend on the scaling of the predictor variables. Standardizing or normalizing the predictors before fitting the Lasso Regression model can help ensure that coefficients are comparable in magnitude.\n",
    "In summary, interpreting the coefficients of a Lasso Regression model involves considering both the magnitude and direction of coefficients while accounting for the sparsity-inducing property of Lasso Regression and the regularization effect. Understanding the impact of predictors on the outcome variable and the resulting sparsity in the model can provide valuable insights for understanding and explaining the relationships captured by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32bee83-f978-4915-8b94-790b7ecab08f",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2ee26b-5f58-4644-bb2f-35b135c8d4f2",
   "metadata": {},
   "source": [
    "In Lasso Regression, the primary tuning parameter that can be adjusted is the regularization parameter (\n",
    "ùúÜ\n",
    "Œª), also known as the penalty parameter. This parameter controls the strength of the L1 penalty applied to the coefficients in the model. The higher the value of \n",
    "ùúÜ\n",
    "Œª, the stronger the penalty, leading to more coefficients being pushed towards zero and more sparsity in the resulting model.\n",
    "\n",
    "Here's how the regularization parameter affects the model's performance:\n",
    "\n",
    "Strength of Regularization:\n",
    "The regularization parameter (\n",
    "ùúÜ\n",
    "Œª) controls the trade-off between the model's bias and variance.\n",
    "Increasing \n",
    "ùúÜ\n",
    "Œª increases the strength of the regularization penalty, which shrinks more coefficients towards zero and increases the sparsity of the model.\n",
    "Stronger regularization leads to a simpler model with fewer predictors, reducing the risk of overfitting but potentially increasing bias.\n",
    "Sparsity:\n",
    "As the regularization parameter (\n",
    "ùúÜ\n",
    "Œª) increases, more coefficients are set to zero, leading to a sparser solution.\n",
    "Sparsity is the property of having many coefficients set to zero, resulting in a simpler and more interpretable model.\n",
    "By tuning \n",
    "ùúÜ\n",
    "Œª, you can control the level of sparsity in the model, selecting a balance between model complexity and performance.\n",
    "Model Performance:\n",
    "The choice of the regularization parameter (\n",
    "ùúÜ\n",
    "Œª) affects the model's predictive performance on both the training and test datasets.\n",
    "Too low a value of \n",
    "ùúÜ\n",
    "Œª may result in overfitting, where the model learns to fit the noise in the training data and performs poorly on unseen data.\n",
    "Too high a value of \n",
    "ùúÜ\n",
    "Œª may lead to underfitting, where the model is too simplistic and fails to capture the underlying patterns in the data.\n",
    "The optimal value of \n",
    "ùúÜ\n",
    "Œª is typically selected through cross-validation or other model selection techniques, balancing between bias and variance to achieve the best generalization performance.\n",
    "In summary, the regularization parameter (\n",
    "ùúÜ\n",
    "Œª) is the main tuning parameter in Lasso Regression, controlling the strength of regularization and the level of sparsity in the resulting model. By adjusting \n",
    "ùúÜ\n",
    "Œª, you can trade off between model complexity and performance, selecting a balance that minimizes overfitting while retaining predictive accuracy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af6c032-dd11-4110-872e-4b18045e253d",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b5ded9-b7da-4ec5-93dd-1a26fab1f65e",
   "metadata": {},
   "source": [
    "\n",
    "Lasso Regression, as a linear regression technique, is inherently designed for linear relationships between predictors and the outcome variable. However, it can be extended to handle non-linear regression problems by incorporating non-linear transformations of the predictor variables. This approach, known as feature engineering or basis function expansion, allows Lasso Regression to capture non-linear relationships between predictors and the outcome variable. Here's how Lasso Regression can be used for non-linear regression problems:\n",
    "\n",
    "Feature Engineering:\n",
    "Non-linear relationships can often be captured by transforming the predictor variables using non-linear functions such as polynomials, logarithms, exponentials, or trigonometric functions.\n",
    "By creating new features that are non-linear transformations of the original predictors, you can introduce non-linearities into the model, allowing Lasso Regression to capture more complex relationships.\n",
    "Polynomial Regression:\n",
    "One common approach to extending Lasso Regression to handle non-linear relationships is polynomial regression.\n",
    "In polynomial regression, the predictor variables are raised to different powers (e.g., squared, cubed) to capture non-linearities in the data.\n",
    "For example, a simple linear model with a single predictor \n",
    "ùë•\n",
    "x can be transformed into a polynomial regression model by including additional terms such as \n",
    "ùë•\n",
    "2\n",
    "x \n",
    "2\n",
    " , \n",
    "ùë•\n",
    "3\n",
    "x \n",
    "3\n",
    " , and so on.\n",
    "Lasso Regression can then be applied to the polynomial features to perform feature selection and regularization.\n",
    "Interaction Terms:\n",
    "Interaction terms, which represent the product of two or more predictors, can also introduce non-linear relationships into the model.\n",
    "By including interaction terms in the model, Lasso Regression can capture non-linear interactions between predictors, allowing for more flexible modeling of complex relationships.\n",
    "Regularization:\n",
    "Even in the presence of non-linear transformations, Lasso Regression retains its regularization property, which helps prevent overfitting and improves the model's generalization performance.\n",
    "Regularization encourages sparsity in the model, favoring simpler solutions with fewer predictors and reducing the risk of overfitting.\n",
    "Cross-Validation:\n",
    "When applying Lasso Regression to non-linear regression problems, it's essential to tune the regularization parameter (\n",
    "ùúÜ\n",
    "Œª) appropriately.\n",
    "Cross-validation or other model selection techniques can be used to select the optimal value of \n",
    "ùúÜ\n",
    "Œª, balancing between model complexity and predictive performance.\n",
    "In summary, while Lasso Regression is originally designed for linear regression problems, it can be adapted to handle non-linear regression problems through feature engineering and basis function expansion. By incorporating non-linear transformations of the predictor variables, Lasso Regression can capture complex relationships and provide flexible modeling capabilities while still benefiting from its regularization properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f7d1c0-83b8-40df-8deb-4f245c84007b",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b886ccc-ad19-4bb9-80e7-0334e2e167be",
   "metadata": {},
   "source": [
    "\n",
    "Ridge Regression and Lasso Regression are both regularization techniques used in linear regression to address overfitting and improve model performance. While they share similarities, such as adding a penalty term to the objective function, they differ primarily in the type of penalty applied and the resulting properties of the models. Here are the key differences between Ridge Regression and Lasso Regression:\n",
    "\n",
    "Penalty Type:\n",
    "Ridge Regression: Ridge Regression adds an L2 penalty term to the ordinary least squares (OLS) objective function. The penalty term is the sum of the squared coefficients multiplied by a regularization parameter (\n",
    "ùúÜ\n",
    "Œª). This penalty encourages smaller coefficient values but does not set any coefficients exactly to zero.\n",
    "Lasso Regression: Lasso Regression adds an L1 penalty term to the OLS objective function. The penalty term is the sum of the absolute values of the coefficients multiplied by a regularization parameter (\n",
    "ùúÜ\n",
    "Œª). Unlike Ridge Regression, Lasso Regression has a sparsity-inducing property that can set some coefficients exactly to zero, effectively performing feature selection.\n",
    "Sparsity:\n",
    "Ridge Regression: Ridge Regression does not lead to sparsity in the model, as it only shrinks the coefficients towards zero without setting any coefficients exactly to zero. All predictors remain in the model, although their coefficients are reduced in magnitude.\n",
    "Lasso Regression: Lasso Regression can produce sparse solutions by setting some coefficients exactly to zero. This sparsity-inducing property makes Lasso Regression well-suited for feature selection, as it automatically identifies and selects the most important predictors while effectively removing irrelevant predictors from the model.\n",
    "Bias-Variance Trade-off:\n",
    "Ridge Regression: Ridge Regression introduces a bias by shrinking the coefficients towards zero, which can help reduce variance and prevent overfitting. It is particularly effective at mitigating multicollinearity and stabilizing coefficient estimates.\n",
    "Lasso Regression: Lasso Regression also introduces a bias by setting some coefficients exactly to zero, leading to a simpler model with reduced variance. The sparsity-inducing property of Lasso Regression makes it suitable for situations where feature selection and interpretability are important considerations.\n",
    "Interpretability:\n",
    "Ridge Regression: Ridge Regression does not perform feature selection and retains all predictors in the model. While it can improve model performance and reduce overfitting, the resulting model may be less interpretable, especially when dealing with a large number of predictors.\n",
    "Lasso Regression: Lasso Regression automatically selects a subset of predictors by setting some coefficients to zero. This leads to a simpler and more interpretable model with fewer predictors, making it easier to understand the relationships between predictors and the outcome variable.\n",
    "In summary, Ridge Regression and Lasso Regression differ primarily in their penalty types and resulting properties of the models. Ridge Regression shrinks coefficients towards zero without setting any exactly to zero, while Lasso Regression can produce sparse solutions by setting some coefficients exactly to zero, performing feature selection in the process. The choice between Ridge Regression and Lasso Regression depends on the specific characteristics of the data and the objectives of the analysis, with Ridge Regression being more suitable for reducing multicollinearity and stabilizing coefficient estimates, and Lasso Regression being more suitable for feature selection and producing interpretable models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a0e2b-b170-4487-afa0-de70f6f5fa21",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c0676-854f-4588-a90c-8fb38bd611b6",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can handle multicollinearity in the input features, although its approach differs from that of Ridge Regression. Multicollinearity occurs when two or more predictor variables in a regression model are highly correlated with each other. Here's how Lasso Regression deals with multicollinearity:\n",
    "\n",
    "Coefficient Shrinkage:\n",
    "Similar to Ridge Regression, Lasso Regression applies a penalty to the coefficients in the model to prevent overfitting. However, Lasso Regression uses an L1 penalty, which has a sparsity-inducing property.\n",
    "The L1 penalty encourages some coefficients to be exactly zero, effectively performing feature selection by selecting a subset of predictors that are most relevant to the outcome variable.\n",
    "In the presence of multicollinearity, Lasso Regression tends to select one of the correlated predictors and set the coefficients of the other predictors to zero. This helps mitigate the effects of multicollinearity by effectively removing redundant predictors from the model.\n",
    "Automatic Feature Selection:\n",
    "Lasso Regression's sparsity-inducing property makes it particularly effective at handling multicollinearity by automatically selecting a subset of predictors.\n",
    "By selecting only one predictor from a group of highly correlated predictors and setting the coefficients of the others to zero, Lasso Regression reduces the impact of multicollinearity on the model's performance.\n",
    "This automatic feature selection can help simplify the model and improve its interpretability while retaining predictive accuracy.\n",
    "Stability of Coefficient Estimates:\n",
    "Lasso Regression can stabilize the coefficient estimates by favoring a simpler model with fewer predictors.\n",
    "By reducing the number of predictors in the model, Lasso Regression helps stabilize the coefficient estimates and reduces their sensitivity to changes in the data, including changes caused by multicollinearity.\n",
    "Regularization Parameter Tuning:\n",
    "The effectiveness of Lasso Regression in handling multicollinearity may depend on the choice of the regularization parameter (\n",
    "ùúÜ\n",
    "Œª).\n",
    "Cross-validation or other model selection techniques can be used to tune \n",
    "ùúÜ\n",
    "Œª and find the optimal balance between model complexity and performance, considering the presence of multicollinearity in the dataset.\n",
    "In summary, Lasso Regression can handle multicollinearity in the input features by automatically performing feature selection and selecting a subset of predictors that are most relevant to the outcome variable. By setting some coefficients to zero, Lasso Regression effectively removes redundant predictors from the model, mitigating the effects of multicollinearity and improving the model's performance and interpretability.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be095057-4f4c-4f00-8b1b-140ec0337028",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a232751c-09e7-4290-97b5-c91eaaa9e978",
   "metadata": {},
   "source": [
    "\n",
    "Choosing the optimal value of the regularization parameter (\n",
    "ùúÜ\n",
    "Œª) in Lasso Regression is crucial for achieving the best balance between bias and variance in the model. Several techniques can be used to select the optimal value of \n",
    "ùúÜ\n",
    "Œª, including:\n",
    "\n",
    "Cross-Validation:\n",
    "Cross-validation is a common technique for selecting the optimal regularization parameter in Lasso Regression.\n",
    "The dataset is divided into training and validation subsets, with different values of \n",
    "ùúÜ\n",
    "Œª tested on the training data, and the performance of each model evaluated on the validation data.\n",
    "Various cross-validation methods, such as k-fold cross-validation or leave-one-out cross-validation, can be employed to estimate the model's performance for different values of \n",
    "ùúÜ\n",
    "Œª.\n",
    "The optimal value of \n",
    "ùúÜ\n",
    "Œª is typically chosen based on the model's performance metrics (e.g., mean squared error, \n",
    "ùëÖ\n",
    "2\n",
    "R \n",
    "2\n",
    " ) on the validation set, selecting the value that minimizes prediction error.\n",
    "Grid Search:\n",
    "Grid search involves systematically testing a range of \n",
    "ùúÜ\n",
    "Œª values over a predefined grid or sequence.\n",
    "For each \n",
    "ùúÜ\n",
    "Œª value, the model is trained using Lasso Regression, and its performance is evaluated using cross-validation.\n",
    "The optimal value of \n",
    "ùúÜ\n",
    "Œª is then selected based on the cross-validation results, choosing the value that yields the best performance metric.\n",
    "Regularization Path:\n",
    "The regularization path, which shows how the coefficients of the predictors change as \n",
    "ùúÜ\n",
    "Œª varies, can provide insights into the optimal value of \n",
    "ùúÜ\n",
    "Œª.\n",
    "By examining the regularization path, one can identify the value of \n",
    "ùúÜ\n",
    "Œª at which certain coefficients transition from being non-zero to zero.\n",
    "The optimal value of \n",
    "ùúÜ\n",
    "Œª is often chosen based on a trade-off between model complexity (number of selected predictors) and performance.\n",
    "Information Criteria:\n",
    "Information criteria, such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC), can be used to select the optimal value of \n",
    "ùúÜ\n",
    "Œª based on model fit and complexity.\n",
    "These criteria penalize model complexity, favoring models that achieve a good fit while using fewer predictors.\n",
    "Validation Set Approach:\n",
    "In situations where computational resources are limited or cross-validation is not feasible, a validation set approach can be used.\n",
    "The dataset is divided into training, validation, and test sets. The model is trained on the training set using various values of \n",
    "ùúÜ\n",
    "Œª, and its performance is evaluated on the validation set.\n",
    "The optimal value of \n",
    "ùúÜ\n",
    "Œª is chosen based on the performance metric on the validation set.\n",
    "Finally, the selected model is evaluated on the test set to assess its generalization performance.\n",
    "In summary, selecting the optimal value of the regularization parameter (\n",
    "ùúÜ\n",
    "Œª) in Lasso Regression involves techniques such as cross-validation, grid search, regularization path analysis, information criteria, and validation set approach. The choice of method depends on the specific characteristics of the dataset, computational resources, and the desired trade-off between model complexity and performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5dbba-5642-4e83-93c0-67addd647f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
